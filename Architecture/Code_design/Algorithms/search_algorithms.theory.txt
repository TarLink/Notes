
                     
   SEARCH ALGORITHMS  
                     



SEARCH KEY ==>                    #Arbitrary starting vertex of search

FINGER SEARCH ==>                 #Search where the search key ("finger") is provided.
                                  #Only efficient if search key is likely to be close to searched node.
                                  #Often implemented on linked lists, treaps, etc.

BRUTE FORCE ==>                   #Trying all combinations|permutations
                                  #Also called "exhaustion".
                                  #Time complexity:
                                  #  - average|worst: O(nᵐ)
                                  #  - best: O(1)
                                  #Space complexity: O(1)
                                  #Can sort input:
                                  #  - so than can stop when > the searched value
                                  #  - to put more likely items first

LINEAR|SEQUENTIAL SEARCH ==>      #Brute force when trying to find an item in an array
                                  #Time complexity: O(n)

PRUNING ==>                       #During a search algorithm, skipping a node's descendants when they
                                  #can be guessed (from the parent's value) to be invalid.
                                  #Unless pruning can be done, search algorithms like BFS/DFS are conceptually a linear search.

BREADTH-FIRST SEARCH (BFS) ==>    #Search one level at a time, i.e.:
                                  #  - start at search key
                                  #  - visit each vertex with distance 1, then 2, etc.
                                  #Time complexity: O(order + size)
                                  #Space complexity: O(order)
                                  #Good when:
                                  #  - depth is big
                                  #  - searched value's height is probably high
                                  #Bad when:
                                  #  - space complexity is important
                                  #  - branching factor is big

DEPTH-FIRST SEARCH (DFS) ==>      #Search one path at a time (until cycle or dead-end), i.e.:
                                  #  - start at search key
                                  #  - visit vertex
                                  #  - if:
                                  #     - it has non-visited children, repeat with the leftmost one
                                  #     - it is the search key, stop
                                  #     - otherwise, repeat with parent ("backtrack")
                                  #Also called "backtracking" when applying to more general algorithms
                                  #"Backjumping":
                                  #  - when it can known that a node and its descendants will be invalid because
                                  #    a siblings|cousin is invalid
                                  #Time complexity: O(order + size)
                                  #Space complexity: O(order)
                                  #Good when:
                                  #  - branching factor is big
                                  #  - searched value's height is probably low
                                  #Bad when:
                                  #  - depth is big

DEPTH-LIMITED SEARCH (DLS) ==>    #DFS where we stop when reaching a given depth.
                                  #Compensate some DFS problems but is an incomplete algorithm

ITERATIVE DEEPENING DFS (IDDFS)   #Running DLS iteratively with increasing depth.
 ==>                              #Also called "Iterative deepening search" (IDS)
                                  #Each iteration repeats DLS on nodes already visited:
                                  #  - cons: time complexity
                                  #  - pro: space complexity, i.e. does not need to store nodes to know if already visited
                                  #I.e. has pros of BFS but without space complexity problem.

LEFT-CHILD RIGHT-SIBLING (LCRS)   #Tree obtained from applying the "Knuth transform" on any rooted directed tree.
 TREE ==>                         #Knuth transform:
                                  #  - make each sibling point to next sibling
                                  #  - only keep edge from parent to first child
                                  #Also called "child-sibling representation", "doubly-chained tree", "filial-heir chain"
                                  #Consequences:
                                  #  - pro: result is a binary tree, which is more space efficient than a k-ary tree
                                  #     - i.e. not knowing each vertex's outdegree requires dynamic arrays, which require
                                  #       extra pointers and dope vectors
                                  #  - con: siblings must be accessed in serial access not random access

SEARCH TREE ==>                   #Ordered arborescence where left children < parent < right children.
                                  #Optimized for:
                                  #  - search
                                  #  - serial access in sorted order
                                  #Time complexity:
                                  #  - search: O(m/2*logₘ n) where m is outdegree
                                  #  - insert|delete:
                                  #     - like search
                                  #     - first search of it, then perform constant-time insert|delete
                                  #  - tree must be balanced, otherwise:
                                  #     - height will average ᵐ√order, and complexity O(ᵐ√n)
                                  #     - can even be O(n) if search tree degenerates into a line graph
                                  #Values can be stored either on each node or only on leaves

SELF-BALANCING TREE ==>           #Tree that remains balanced after any set of operations
                                  #Also called "height-balancing tree"
                                  #Often on search trees

BINARY SEARCH TREE (BST) ==>      #Search tree where outdegree = 2
                                  #Search is "binary search".
                                  #Time complexity (if balanced): O(log n)

B-TREE ==>                        #Search tree where:
                                  #  - ⌈U/2⌉ <= outdegree <= U ("order")
                                  #  - no lower limit of outdegree for root
                                  #  - node's value is an array or numbers:
                                  #     - acting as separators between each two children
                                  #  - U can be picked to try to make nodes the same size as an I/O block
                                  #Similar to binary search tree:
                                  #  - which can be considered a b-tree of order 2
                                  #The higher the order:
                                  #  - better when nodes are read|write sequentially
                                  #     - i.e. when siblings nodes are usually read|write together
                                  #     - e.g. a file's blocks
                                  #  - instead of O(log₂ n), performs in O(m/2*logₘ n) to find node
                                  #    (which is worst), but then iterating|inserting|deleting siblings is faster
                                  #  - worst time complexity: some space is empty
                                  #  - less frequent balancing needed
                                  #If U is very high, can:
                                  #  - implement keys as a search tree themselves instead of a simple array
                                  #  - use delta encoding compression on keys (better space, worst time)
                                  #Self-balancing:
                                  #  - if node full|empty, can:
                                  #     - transfer ("rotation") or "merge|split" between siblings
                                  #        - must change parent keys as well
                                  #     - merge|split child to|from parent
                                  #  - must be done recursively from node to root
                                  #  - can be done either:
                                  #     - between operations
                                  #     - during operation, as nodes are being visited

B+ TREE ==>                       #B-tree where:
                                  #  - values are stored only on leaves
                                  #  - each leaf contains a pointer to next leaf (i.e. linked list)
                                  #I.e. like B-tree but with better sequential access accross several leaves.

RANDOM SEARCH TREE ==>            #When shuffling nodes before inserting many of them in a search tree.
                                  #Reason:
                                  #  - lower depth in average
                                  #  - e.g. inserting many sorted nodes (without balancing) would otherwise
                                  #    create a graph list as a subtree, increasing depth linearly
                                  #Variants:
                                  #  - do a random pick among all possible trees after insertion of each
                                  #    permutation of the nodes.
                                  #For a binary search tree, in average, depth will be:
                                  #  - mean: 2*log(n)+1
                                  #  - max: 4.3*log(n)

TREAP ==>                         #Tree of [VAL, KEY] pairs:
                                  #  - VALs behave like a search tree, usually binary
                                  #  - KEYs behave like a heap
                                  #  - self-balanced by making it a random search tree (using random KEYs)
                                  #Goal is self-balancing a search tree
                                  #Self-balance:
                                  #  - insertion:
                                  #     - perform binary search tree insertion using VAL
                                  #     - assign a random KEY
                                  #     - perform tree rotations on that node and its parent until KEY satisfies heap property
                                  #  - deletion:
                                  #     - perform tree rotations on that node and its right child until that node has no children
                                  #     - delete that node
                                  #  - split:
                                  #     - insert a node with VAL where split occurs and KEY being highest so that node becomes
                                  #       root
                                  #     - then remove that node, splitting tree
                                  #  - merge:
                                  #     - if max VAL of first tree if less than min VAL of second tree
                                  #     - then insert a node with VAL2 in-between, and KEY being lowest so that rotations
                                  #       performs the merge
                                  #     - then remove that node

TRIE ==>                          #Search tree where:
                                  #  - keys are arrays
                                  #  - nodes store individual keys' items
                                  #     - i.e. root-to-terminal-node path contains full key
                                  #     - root node has no key item
                                  #Also called "prefix tree"
                                  #Goal:
                                  #  - similar as other search trees but optimized for operations of arrays
                                  #  - only efficient when keys share common prefixes
                                  #Time complexity:
                                  #  - min|average|max O(m) where m is min|average|max array length
                                  #  - but each node step should be fast as it involves a single key item
                                  #     - as opposed e.g. to binary search tree which compares whole array at
                                  #       each step, i.e. complexity is actually O(n*log(n)) on arrays
                                  #Keys are arrays, including:
                                  #  - strings
                                  #  - bits ("bitwise trie")
                                  #Terminal node:
                                  #  - every leaf is a terminal node but internal nodes can be too
                                  #  - can be represented either:
                                  #     - with a flag on the node
                                  #        - sometimes called "white" (terminal) / "black" (non-terminal)
                                  #     - with an empty terminal node child
                                  #Edges from parent to children can be represented in several ways:
                                  #  - dynamic array ("adaptive"):
                                  #     - O(n) lookup time
                                  #  - LCRS tree:
                                  #     - O(n) lookup time but more space efficient
                                  #  - binary search tree:
                                  #     - called (although confusingly) "ternary search tree"
                                  #     - O(log n) lookup time
                                  #     - often represented as a tree with outdegree 3 where:
                                  #        - left and right children are siblings (i.e. the binary search subtree)
                                  #        - middle child is actual child (i.e. the trie supertree)
                                  #  - static array:
                                  #     - with every possible combination, filled with null pointers when no child
                                  #     - O(1) lookup time but O(m) space requirement, where m is number of combinations
                                  #     - "alphabet reduction":
                                  #        - reducing number of combinations, e.g. nibble-wise (16) instead of byte-wise (256)
                                  #        - con: increases tree height
                                  #     - "array mapped trie": when using "array bit mapping" (see its doc)
                                  #        - "concurrent trie"/"ctrie":
                                  #           - using a 32 bits map and a compare-and-swap atomic CPU instruction, so that
                                  #             trie can be thread-safe and lock-free
                                  #Operations:
                                  #  - finding nodes by key prefix

RADIX TREE ==>                    #Trie where nodes can store several key items, as opposed to just one.
                                  #Goal:
                                  #  - more space efficient
                                  #  - more time efficient on reads
                                  #Cons:
                                  #  - less space efficient if high density
                                  #  - less time efficient on writes
                                  #Operations:
                                  #  - insertion: during initial traversal, if there is a partially matching child, split it
                                  #  - deletion: after deletion, merge single-child nodes to their child

DAFSA ==>                         #DAFSA (see state doc) can used to represent tries in a more space efficient way.
                                  #As opposed to normal representation:
                                  #  - simple cycles (but not directed ones) are allowed
                                  #  - i.e. nodes can share multiple parents as long as it does not introduce directed cycles
                                  #  - i.e. space efficient when many common suffixes

BURST TRIE ==>                    #Trie ("access trie") where leaves ("burst") are another data structure ("burst containers").
                                  #Goal:
                                  #  - using time|space efficiency of trie where prefixes are shared
                                  #  - while using time|space efficiency of other data structures where suffixes are unique
                                  #Common burst containers: list, search tree.
                                  #Different strategies on when burst should start:
                                  #  - max size of burst containers
                                  #  - max average number of operations on burst container

HEAP ==>                          #Tree satisfying the "heap property":
                                  #  - "max heap": parent node's key >= child's key
                                  #  - "min heap": parent node's key <= child's key
                                  #Goal:
                                  #  - performing min|max
                                  #  - e.g. implementing a priority queue
                                  #Operations:
                                  #  - peek_max|min(): O(1)
                                  #  - pull_max|min(): O(log n)
                                  #  - insert(NODE): O(log n)
                                  #  - delete(): arbitrary node
                                  #  - set_key(NODE, KEY): O(log n)
                                  #  - merge(HEAP, HEAP2)->HEAP3: O(n)
                                  #  - size()->NUM
                                  #Complexity depends on implementation (those are the most common).

HASH TABLE ==>                    #Implementation of an associative array where:
                                  #  - values are stored in an array
                                  #  - keys are hashed and used as array indices ("buckets")
                                  #     - "keyspace": all possible keys
                                  #Also called "hash map"
                                  #Array size N:
                                  #  - often smaller that hashes domain, in which case hashes are
                                  #    reduced using the modulo operator.
                                  #Collisions:
                                  #  - number of collisions:
                                  #     - min: ⌊n/N⌋
                                  #     - average: n/N
                                  #     - max: n
                                  #  - collision resolution schemes:
                                  #     - increasing array size
                                  #     - extendible hashing
                                  #     - separate chaining
                                  #     - open addressing
                                  #     - perfect hashing
                                  #Pros:
                                  #  - time complexity of operations (search, insert, delete): O(1)
                                  #Cons:
                                  #  - unordered
                                  #  - space efficiency O(m) where m are possible hashes
                                  #  - time cost of the hashing function
                                  #     - i.e. not efficient if small number of keys
                                  #  - time|space cost of handling collisions
                                  #  - poor locality of reference

HASH TABLE RESIZING ==>           #"All-at-once rehash":
                                  #  - triggered on specific density threshold
                                  #  - re-inserting each element, recalculating each hash
                                  #"Incremental rehash":
                                  #  - build the new array in parallel
                                  #     - either time-based, or incrementally after each operation
                                  #  - check both old and new array on search|delete
                                  #  - use only new array on insert
                                  #"Linear hashing":
                                  #  - divide into three parts 1), 2) and 3)
                                  #     - when hashes falls into 1), use twice modulo instead
                                  #        - which means it could fall into either 1) or 3)
                                  #  - adding a single bucket:
                                  #     - added to 3)
                                  #     - first 2) bucket becomes 1)
                                  #        - bucket re-hashed ("split") with new modulo
                                  #        - point between 1) and 2) is the "split pointer"
                                  #  - if 2) has no more buckets:
                                  #     - 1) and 3) merged into 2)
                                  #  - i.e. rehash is done incrementally
                                  #  - only works with separate chaining
                                  #Distributed hash tables (see below) have better time complexity

EXTENDIBLE HASHING ==>            #Way to handle collisions in a hash table.
                                  #How:
                                  #  - each bucket has its own modulo size ("local depth")
                                  #     - initially same as global depth
                                  #  - "global depth":
                                  #     - hash table size
                                  #     - resized by a factor of D (often 2)
                                  #On insert collision:
                                  #  - if bucket's local depth < global depth:
                                  #     - split it into D buckets
                                  #     - 1 bucket will be full, the others empty
                                  #     - repeat until either no collision or local depth = global depth
                                  #  - if still collision:
                                  #     - resize hash table
                                  #        - i.e. global depth is increased but not buckets' local depth
                                  #        - i.e. O(1) time complexity
                                  #     - then re-insert
                                  #Often implemented using a hash trie (called "directory")
                                  #  - in which case, local depth is also number of pointers from trie to bucket

PERFECT HASHING ==>               #Way to handle collisions in a hash table.
                                  #Choosing a hashing function (i.e. universal hashing function) that creates no collision
                                  #  - must try several hashing function until find one
                                  #"FKS hashing":
                                  #  - doing hashing twice, and using a hash iliffe vector instead
                                  #  - O(n²) space complexity, but much higher chance to find hashing functions
                                  #Set of keys can be:
                                  #  - static
                                  #  - dynamic: must rebuild and|or resize either:
                                  #     - when collision happens
                                  #     - after specific number of operations
                                  #Comparison with separate chaining:
                                  #  - pro: very time efficient
                                  #  - con: building is very slow, so set of keys should be mostly static

[SEPARATE] CHAINING ==>           #Way to handle collisions in a hash table
                                  #Storing several values ("overflow") as a data structure, instead of a single value
                                  #Often used data structures:
                                  #  - linked list:
                                  #     - can either store a pointer to a linked list, or directly a linked list ("list
                                  #       head cells")
                                  #  - dynamic array ("array hash table")
                                  #  - search tree: only makes sense when hashing function is highly not uniform

OPEN ADDRESSING ==>               #Way to handle collisions in a hash table.
                                  #Also called "closed hashing"
                                  #How:
                                  #  - on write collision (value already exists) or read collision (value does not match)
                                  #  - iterates to next element ("probing" through a "probe sequence") until one fits
                                  #     - cycles from end to start
                                  #  - possible schemes are below
                                  #Clustering:
                                  #  - when values are closed to each other, causing collisions
                                  #  - increases with high density
                                  #  - can be:
                                  #     - "primary":
                                  #        - when probing sequence is close to each other (e.g. linear probing)
                                  #        - amplified when hash function is not uniform
                                  #     - "secondary":
                                  #        - when probing sequence is far from each other (e.g. double hashing)
                                  #        - problematic when resizing
                                  #  - probe sequences overlap if using the same (from worst to best):
                                  #     - current bucket, i.e. can overlap even on different key and key hash
                                  #     - first bucket, i.e. overlaps if key hash is same
                                  #     - key, i.e. no overlap
                                  #  - when probing iteration is determined only by current item:
                                  #     - i.e. not first one nor input
                                  #     - collisions will make probe sequences overlap, amplifying the problem
                                  #Time complexity:
                                  #  - O(1 + n/N), i.e. density, i.e. probability of collision with clustering
                                  #  - high locality of reference improves cache performance
                                  #     - i.e. if values are too large to fit in CPU cache, open addressing is less interesting
                                  #Deletion:
                                  #  - must copy the next values (according to iteration) to fill the iteration gap
                                  #  - "lazy deletion":
                                  #     - deleted values are just flagged instead of erased from memory
                                  #     - inserting into a deleted-flagged value simply overwrite it
                                  #     - searching through a deleted-flagged value makes it iterate to next value, but the
                                  #       finally found value is then copied to the deleted-flagged value
                                  #        - can also do this as part of a cleanup routine, e.g. after a specific number of
                                  #          operations
                                  #     - pro: faster deletion
                                  #     - con: slower search
                                  #Comparison with separate chaining:
                                  #  - pros:
                                  #     - more space efficient
                                  #     - more time efficient when low density because fewer pointers
                                  #  - cons:
                                  #     - less time efficient when high density because of clustering, i.e.:
                                  #        - need to dynamically resize on smaller density threshold
                                  #        - stronger need for uniform hash function
                                  #     - it is possible to run out of values if no resize is done

K-CHOICE HASHING ==>              #Using k different probing methods|sequences.
                                  #On insertion, picking the one with the least number of collisions.

LINEAR PROBING ==>                #Fixed increment, usually 1
                                  #Stops iteration when going back to same position
                                  #Pros:
                                  #  - low secondary clustering
                                  #  - high locality of reference
                                  #Cons:
                                  #  - high primary clustering
                                  #  - probe sequence overlaps on current bucket

ROBIN HOOD HASHING ==>            #Similar to linear probing except:
                                  #  - on final insertion, the number of iterations performed is stored
                                  #  - during iteration, if the new item's number of iterations > current item's stored number,
                                  #    swap them
                                  #Goal: spread the lengths of probe sequences, reducing worst time complexity.

QUADRATIC PROBING ==>             #Quadratic polynomial increment, e.g. x²
                                  #Stops iteration when x is bigger than array size
                                  #  - x might repeat itself though:
                                  #     - at least first x/2 items will be unique if either:
                                  #        - array size is prime
                                  #        - array size is 2ᵐ and increment is n*(n-1)/2
                                  #     - all items will be unique if:
                                  #        - array size is prime and 3+m*4, and increment is alternating between x² and -x²
                                  #  - i.e. important to keep low density
                                  #Pros and cons:
                                  #  - in-between linear probing and double hashing
                                  #  - probe sequence overlaps on first bucket

DOUBLE HASHING ==>                #Increment is HASHₓ(key), using a series of universal hash functions
                                  #Iteration keeps going until found
                                  #Pros:
                                  #  - low primary clustering
                                  #  - no probe sequence overlaps
                                  #Cons:
                                  #  - high secondary clustering
                                  #  - low locality of reference
                                  #  - requires extra hash computation

CUCKOO HASHING ==>                #Similar to double hashing except:
                                  #  - HASHₓ(key) is location not increment
                                  #  - instead on inserting on last location found, insert on first one and push other
                                  #    values one position forward
                                  #  - uses limited number m of hash functions:
                                  #     - on loops, must trigger an array resize
                                  #     - often divide array in m, with each slice having its own hash function
                                  #     - when m is smaller:
                                  #        - pro: better worst time complexity
                                  #        - con:
                                  #           - higher probability of cycles, i.e. density must be smaller
                                  #           - m = 2 -> density < 0.5, m = 3 -> density < 0.91
                                  #Comparison with double hashing:
                                  #  - worst average time complexity
                                  #  - better worst time complexity (because resize is triggered more often)

COALESCED HASHING ==>             #Hybrid with separate chaining
                                  #On collision:
                                  #  - uses the first empty bucket on the table
                                  #     - its position is stored to avoid linearly searching it each time
                                  #  - use pointers, i.e. form linked list inside the table
                                  #"Cellar":
                                  #  - optimization where part of the hash table is reserved for collision values
                                  #  - pro: faster to find next empty bucket
                                  #  - con: must use heuristic to find good ratio, often 0.14
                                  #Deletions are slow

DISTRIBUTED HASH TABLE ==>        #Hash table distributed over several nodes:
                                  #  - nodes communicate using an "overlay network"
                                  #  - keyspace is split among nodes using a "keyspace partioning" scheme
                                  #Goals: scalability, decentralization, fault tolerance

CONSISTENT HASHING ==>            #Keyspace partioning scheme, where:
                                  #  - each node gets assigned as value a random position ("token") in keyspace
                                  #  - each key belongs to the node with the closest greater|lesser node value
                                  #Also called "stable hashing"
                                  #"Weighting":
                                  #  - assigning several values per node, to ensure better spread
                                  #Resizing:
                                  #  - on node removal, its keys are reassigned
                                  #  - on node insertion, its keys are calculated, reassigned from other nodes
                                  #  - i.e. in average, node removal|insertion move N/n keys, where N is number
                                  #    of keys and n number of nodes

RENDEZVOUS HASHING ==>            #Keyspace partioning scheme, where:
                                  #  - each node gets assigned a random value
                                  #     - total order over all nodes can also be set in case of score ties
                                  #  - each key belongs to the node with the lowest HASH(key, node_value) ("score"/"weight")
                                  #     - can also pick the n lowest scores, to assign several nodes per key
                                  #Resizing is similar to consistent hashing
                                  #Pro:
                                  #  - more distributed, less clustered keyspace
                                  #Con:
                                  #  - need to iterate over each node, instead of just the closest one
                                  #     - i.e. O(n) time complexity where n is number of nodes

HASH LIST ==>                     #Result of hashing a list of values, where each value gets its own hash.
                                  #Goal:
                                  #  - incremental hashing write|read
                                  #  - e.g. verifying integrity incrementally
                                  #"Top|root|master hash":
                                  #  - single hash of all hashes
                                  #  - goal:
                                  #     - total (i.e. non-incremental) hashing write|read
                                  #     - e.g. verifyint integrity of hash list
                                  #"Hash|Merkle tree":
                                  #  - like top hash but using a tree of hashes where each parent hashes its children
                                  #     - often uses binary tree
                                  #  - should restrict specific depth and branching factor:
                                  #     - otherwise second pre-image attacks are easier to forge
                                  #     - can alternatively prepend some fixed byte to each value and some other byte to each
                                  #       internal hash
                                  #  - pro:
                                  #     - can compute single bottom-level hash to top hash with fewer operations
                                  #     - i.e. faster computation: O(log n) instead of O(n)
                                  #     - also does not need all hashes to be known, just O(log n) of them
                                  #  - con:
                                  #     - inverse when it comes to computing all bottom-level hashes at once
                                  #     - building tree is O(n*log n)

HASH CHAIN ==>                    #List of hashes where each node hashes the previous hash.
                                  #Goal:
                                  #  - increasing time complexity to verify nodes from O(1) to O(n)
                                  #  - e.g. can be used to generate one-time keys (against eavesdropping)

BINARY HASH CHAIN ==>             #Hash chain where each hash also gets some extra input
                                  #Goal:
                                  #  - incremental hashing write|read
                                  #  - similarly to a hash tree, but with:
                                  #     - lower time complexity to add nodes: O(1)
                                  #     - higher time complexity to verify nodes: O(n)

HASH TRIE ==>                     #Hash map where hashes are stored in a trie.
                                  #Goal:
                                  #  - slightly lower time complexity of hash map
                                  #  - but with much lower space complexity
                                  #  - better persistent data structure
                                  #"Hash array mapped trie" (HAMT): when using array mapped trie

BLOOM FILTER

Implementation of a set
Giant hash bit-or'd with each element, of size m
Hashes must have small number k of 1-bits
  - can do it by using k hash functions that map to [0-m-1]
Space efficient
Time complexity: O(k)
Can only do add() and has(), not remove(), iterate, count, union, etc.
Possible false positive (but no false negative)
If set elements is known, bit array is more efficient

Variant:
  - using a second bloom filter for elements that have been removed
  - allow remove operation
  - but introduce possible false negatives

RANGE QUERY ==>                   #Performing an operation over a subset of an array
                                  #Operation can be:
                                  #  - minimum|maximum ("range minimum|maximum query" (RMQ))
                                  #  - mode ("range mode query")
                                  #  - median

CARTESIAN TREE ==>                #Heap built from a sequence of keys, that can be reversed to that sequence of keys with
                                  #an in-order traversal.
                                  #Construction:
                                  #  - time complexity: O(n)
                                  #  - from "all nearest smaller values" algorithm:
                                  #     - going in both directions, i.e. building left|right neighbors
                                  #     - if has no left|right neighbors, is root
                                  #     - if higher neighbor is left|right, put as right|left child of it
                                  #Goal is efficient range minimum|maximum query:
                                  #  - minimum|maximum of any subset of the original sequence of keys
                                  #    is the lowest common ancestor to the first and last keys of the subset
                                  #  - time complexity: O(1)
                                  #  - space complexity: O(n) (storing the tree itself)

ALL NEAREST SMALLER VALUES ==>    #Algorithm:
                                  #  - in a sequence of values
                                  #  - for each value:
                                  #     - lookup all the previous values
                                  #        - sometimes done in both directions
                                  #     - find the indices of the closest lower|higher value
                                  #Example: [0, 8, 4, 9] -> [null, 0, 0, 2]
                                  #Can be efficiently implemented as a parallel algorithm.
