
              
   ALGORITHMS  
              



QUALITIES ==>                     #Qualities of an algorithm:
                                  #  - "optimality": finds the best solution
                                  #  - "precision": results are close to each other
                                  #  - "accuracy": results are close to solution
                                  #  - "completeness": all results are found among possible solutions
                                  #  - "efficiency": time and space complexity

POSITIVE/NEGATIVE ==>             #"Condition positive|negative" (CP|CN):
                                  #  - number of positive|negative possible solutions
                                  #  - "prevalence": CP/(CP+CN)
                                  #"Predicted condition positive|negative":
                                  #  - number of positive|negative results
                                  #  - can be:
                                  #     - "true positive" (TP)/"statistical power"
                                  #     - "true negative" (TN)
                                  #     - "false positive" (FP)/"type I error"/"false alarm"
                                  #     - "false negative" (FN)/"type II error"/"miss"
                                  #Predicted condition:
                                  #  - % of positives|negatives results that are true|false
                                  #  - ratios:
                                  #     - "precision": TP/(TP+FP)
                                  #        - also called "positive predictive value" (PPV)
                                  #        - % of positives that are true
                                  #        - inverse: "false discovery rate" (FDR), FP/(TP+FP)
                                  #     - "negative predictive value" (NPV): TN/(TN+FN)
                                  #        - % of negatives that are true
                                  #        - inverse: "false omission rate" (FOR), FN/(TN+FN)
                                  #True condition:
                                  #  - % of positive|negatives results among all possible solutions
                                  #  - ratios:
                                  #     - "completeness": TP/CP
                                  #        - % of positive results among all possible solutions
                                  #        - also called "recall"/"sensivity"/"probability of detection"/
                                  #          "true positive rate" (TPR)/"statistical power"
                                  #        - inverse: "miss rate"/"false negative rate" (FNR)/"false alarm ratio" / β, FN/CP
                                  #     - "specificity": TN/CN
                                  #        - % of negative results among all possible solutions
                                  #        - also called "true negative rate" (TNR)
                                  #        - inverse: "fall-out"/"false positive rate" (FPR) / "significance level" / α, FP/CN
                                  #     - "positive likelihood ratio" (LR+): TPR/FPR, i.e. (TP*CN)/(FP*CP)
                                  #     - "negative likelihood ratio" (LR-): FNR/TNR, i.e. (FN*CN)/(TN*CP)
                                  #     - "diagnostic odds ratio" (DOR): LR+/LR-, i.e. (TP*TN)/(FP*FN)
                                  #     - "F₁ score": 2/(1/PPV + 1/TPR), i.e. combine PPV and TPR

HEURISTIC ==>                     #Achieving high efficiency by trading off other qualities
                                  #"Greediness":
                                  #  - heuristic using iteration:
                                  #     - each iteration picks candidate ("local optimum") that gets closer to solution
                                  #       ("global optimum")
                                  #     - if no candidate, stops

RECURSION ==>                     #Algorithm that calls itself
                                  #Can be:
                                  #  - "divide and conquer":
                                  #     - start with subset of input, merged into bigger input
                                  #     - example: distribution sorts
                                  #  - "decrease and conquer":
                                  #     - start with whole input, divided into subsets
                                  #     - example: binary search
                                  #     - "prune and search": when doing it with constant factor
                                  #Pros:
                                  #  - parallelism
                                  #  - smaller problem
                                  #Cons:
                                  #  - stack size

PARALLELISM ==>                   #See parallelism doc
