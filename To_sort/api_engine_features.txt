Pagination:
  - token order|filter:
     - should be raw ones
     - should only be used for comparison with current query
     - topargs should be used for actual work
     - should adapt minification
  - top-level action:
     - findMany|patchMany:
        - use runOpts.pagesize INT
           - instead of the three runOpts currently used, i.e. maxPageSize, defaultPageSize and maxDataLength
        - def 100
        - 0 to disable pagination:
           - no paginating
           - no output metadata
           - passing pagination args is a noop, but not an error
        - args.pagesize can change it but must be > 0 and <= runOpts.pagesize
     - findOne|patchOne|delete*|upsert*|create*: no pagination
  - offset and cursor pagination:
     - for top-level pagination
     - is offset pagination if args.page !== undefined
     - default is cursor pagination otherwise, with args.after ""
     - document difference with offset pagination: not random-access, but stronger consistency/isolation
  - offset pagination:
     - findMany only, not patchMany
     - args:
        - page, pagesize
     - metadata.pages:
        - page, pagesize
        - has_previous|next_page BOOL
        - total_size NUM, pagecount NUM:
           - calculated by doing an extra count database query
           - always performed, but on offset pagination only
  - cursor pagination:
     - args:
        - pagesize
        - after, before: same as current system, including:
           - how cursors are calculated
           - "" cursors for beginning and end
     - metadata.pages:
        - pagesize: the one that was actually used
        - has_previous|next_page BOOL
        - previous|next_token "CURSOR":
           - instead of current system where each model has its own token
           - those two cursors are the cursor calculated of the last and first model
           - not defined if has_previous|next_page false
           - clients should submit the same args.filter|order_by (instead of not submitting any, as it is now),
             i.e. only add args.after|before.
        - first|last_token: always ""
           - not defined if has_previous|next_page false
        - total_size NUM, pagecount NUM:
           - like offset pagination, except only set on nested findMany actions, where it can be guessed without
             additional queries
        - first|last|previous|next:
           - this is REST-specific
           - like first|last|previous|next_token, but prepended with URL to perform the query
           - also as HTTP Link: <URL>; rel="first|last|prev|next|self[ ...]"
     - patchMany only has forward cursors:
        - no args.before nor metadata.pages.previous|first|last_token
        - metadata.pages.has_previous_page is always false
  - pagination metadata:
     - at response.metadata.pages.* for top-level action
     - at response.metadata.pages.nested.ATTR..* for nested actions
  - nested action:
     - findMany:
        - use runOpts.maxmodels INT:
           - must calculate nestedpagesize that:
              - is the same across all nested actions
              - is as high as possible while still making whole response <= maxmodels
              - is an integer, i.e. is rounded down
              - is >= 1
                 - if cannot, e.g. maxmodels is very small, throw client-side error
              - is <= pagesize, to avoid circumventing pagination
           - since nested actions are known (because ids are fetched first):
              - paginated at API level
              - we can estimate it:
                 - since each top-level action has its own number of descendants, this might require an algorithm that
                   does several trials under maximized.
           - def: 100 * runOpts.pagesize (not args.pagesize)
        - nested pagesize is applied on each nested action, from a client perspective, even though the server concatenate
          all of them in a single command
     - delete:
        - no pagination, including runOpts.maxmodels
        - if dryrun, paginated like find command instead:
           - including response metadata
     - create|upsert:
        - throws if args.data has recursively more than runOpts.maxmodels
     - patch:
        - throws if response is recursively more than runOpts.maxmodels:
           - do this by checking, before any command, the cumulated current results (including pending results) length,
             adding the current command length, and check if > maxmodels
  - nested actions restrictions:
     - find: can only do a *many action top-level or at the second level
     - patch|delete|create|upsert: can do *many action on any level
  - duplicates:
     - pagination, including maxmodels, does not care whether models are duplicate only. In other words, maxmodels is
       checked against modelscount not uniquecount.
  - limits:
     - increase maxpayloadsize default to 10MB
     - MAX_DEPTH 5:
        - including top-level
        - cannot be configured by user
  - remove system defaults middleware
     - move all of them to their respective middleware
  - user document:
     - batch size might be smaller than args.pagesize (e.g. if response duplicates were removed)
  - relay.js:
     - requires model-wise cursors, which is too slow, and too complicated for users, i.e.:
        - only expose Relay.js cursors as opt-in
        - only first and last cursor are calculated. The others simply append an offset to the first cursor.
           - it is not as isolation-safe, but is much faster and only meant for Relay.js

Errors:
  - status:
     - errors during schema compilation or server startup should have HTTP status code 0
  - normalize error reasons:
     - should be not about guessing where error was internally triggered, but in terms of how client should respond.
     - e.g. it does not matter if InputValidation error is because args.data is wrong or args.filter, what matters is that
       client displays that input is wrong.
     - differentiate between server-side errors caused by apiengine bug or by maintainer bug (e.g. schema function bug)
        - including system patch operator vs custom patch operator
  - simplify validation:
     - there are too many validation sub-systems. Simplify all this
     - use more of JSON validation, but pre-compile JSON schemas to keep it fast
  - normalize error `extra`:
     - use utilities like the ones for AUTHORIZATION (throwAuthorizationError()):
        - one per error reason
        - i.e. force specific message and specific `extra` per error reason
     - add `extra` information to any throw ERROR that possess interesting information
     - all errors with the same `reason` should expose the same `extra` variables
     - example of `extra`:
        - data that failed
        - data path: either as JSON path (def) or as JSON pointer (using a runOpt)
           - set GraphQL errors's `path` with this, and possibly other documented in my GraphQL doc
        - rule (e.g. 'exclusiveMinimum')
        - `location.line|column` and `position` in bytes of input
           - set GraphQL errors's `locations|positions|source|nodes` with this, and possibly other documented in my GraphQL doc
  - add all `generic.title`
  - check new AJV versions
  - error messages:
     - improve error messages for JSON schema composed types: schema.contains|propertyNames|not|anyOf|oneOf|if|then|else
       (others are fine)
  - user documentation:
     - available error responses types, error `extra` and status codes
  - reporting several error at once:
     - e.g. several data validation issues
     - possible idea:
        - when several errors can arise (e.g. data validation), wait until all are known to throw error
        - add as ARR field in error.extra.*
  - correction suggestions:
     - e.g. wrong attribute name or wrong args.filter operator
     - provide list both in error message, and as parsable `extra` info
     - if possible, use levenshtein distance to guess possible typo suggestion
  - request timeout error includes very little information (e.g. does not include protocol) because it is fired in
    an early middleware
  - variants: if data validation fails because of an attribute that was generated as a variant, error message must
    include information that it was generated as such, otherwise it's confusing for end-user

inputValidation middleware:
  - like dataValidation middleware but before any server-side data transformation:
     - i.e. beginning of command layer
     - reuse attr.validation like dataValidation middleware
        - reason we do not have two different sets of validation: data stored in database should be valid input, and also having two sets adds two much complexity and room for errors to end-users
     - gives 4** client-side errors, while dataValidation middleware gives 5** server-side errors
     - input only (like dataValidation middleware)
     - should try to memoize|reuse per request (to avoid memory leaks) between inputValidation and dataValidation middlewars

JSON schema $data:
  - $data notation is a bit cryptic and prone to error with JSON pointers.
     - should replace to something more user-friendly, where user just need to specify the sibling model's attribute name
     - at the moment, $data is removed before schema validation, this would need to be changed
     - validateMap would need to convert this notation to $data, for ajv to work
     - error reporting does not currently work with $data, because ajv does not translate $data into the actual referred
       data. E.g. it says "should equal [Object object]" because { $data: STR } is kept as is

Aggregation:
  - args.group:
     - in findMany commands
     - value "ATTR,...":
        - ATTR value can be any type, including undefined
        - if several ATTR,... group by ordered union of all of them
           - i.e. GROUP is an ARR
        - should validate against bad format and unknown attributes, just like args.order
        - should be parsed in parseGroupBy middleware, after args.parseOrderBy middleware (since it modifies it)
     - performed just before parseResponse middleware:
        - i.e. has no impact on any other middleware
        - i.e. done at API level, not database adapter
     - transforms response:
        - from OBJ_ARR to OBJ2_ARR: group GROUP[_ARR], items OBJ_ARR
     - args.order:
        - prepend args.group to args.order, so that response is sorted by GROUP values, even when paginated.
        - if some attributes in args.group were already in args.order, remove them first
     - do not allow grouping on an ATTR that has unique constraint
        - including model.id
  - args.aggregate:
     - on findMany commands
     - value { VAR: AGG, ... }
        - each AGG is run and assigned to each VAR, returning an OBJ
        - AGG is "FUNC [...]", with available ones:
           - count [ATTR,...]:
              - count models where no ATTR == null (by augmenting args.filter) or all models if no ATTR specified
           - sum|avg|min|max VAL:
              - VAL can contain:
                 - ATTR:
                    - including multiple ones
                    - when specifying ATTRs, only include models where no ATTR == null, by augmenting args.filter
                 - + - * /
                 - ( )
                 - NUM finite constant
              - tokenize then parse to AST in parseAggregate middleware, in action layer
                 - it should validate against unknown ATTRs too
                 - AST:
                    - type: "count" (ATTR,... is handled by augmenting args.filter)
                    - type: "sum|avg|min|max", value NODE
                    - type: "add|substract|multiply|divide", left NODE, right NODE
                    - type: "attribute", value "ATTR"
                    - type: "constant", value NUM
     - args.aggregate AST is passed to database adapter, which performs the aggregation
     - database feature "aggregate:count|min|max|avg|sum"
     - args.select should assume response will look like args.aggregate, instead of using model schema:
        - since args.aggregate, this also imply args.select will never have nested response/actions
     - if args.aggregate !== undefined, the following is not applied:
        - pagination:
           - also forbid pagination-related args
        - whole response layer:
           - including validateMissingIds (but not authorization middleware), i.e. no 404 will be checked
        - renameIdsOutput middleware
     - forbid args.order
  - args.aggregate + args.group:
     - aggregate is performed on each group
        - response looks like OBJ_ARR: group GROUP[_ARR], AGG_VAR: VAL (no "items")
     - args.group_filter:
        - like args.filter, except applied after grouping
        - parsed in same middleware as args.filter
        - forbidden if args.group or args.aggregate undefined
     - args.order:
        - is applied after grouping
        - i.e. just like args.select, attributes must be validated against how aggregated response looks like,
          not usual response
     - pagination:
        - cursor pagination is merged to args.group_filter, not args.filter
        - offset pagination, i.e. offset|limit is applied after grouping
     - everything else behaves same as args.aggregate without args.group, except:
        - args.select amd results merging must be applied on each group
        - args.order and pagination are allowed
     - database feature "aggregate:group"
     - args.aggregate can be empty, if want to obtain a result similar to SQL "distinct"

Static assets:
  - take inspiration from existing ones, probably reusing one
  - take inspiration from Express sendFile()
  - think of Content-Disposition
  - integrate GraphiQL with this
  - server-side templates serving
     - including isomorphic server-side renderer

Orphans:
  - maybe keep dead links dangling???
     - but either:
        - silently filter them on find
        - also remove them (in a separate thread|promise) on find
     - what if validation fails???
        - request fails???
        - model is deleted???
  - is it worth the performance penalty and the validation constraints???
     - although delete probably do want to remove dead links anyway???
     - removing dead links when being populated???
     - removing dead links in a separate promise, e.g. not affecting current query???
  - validation:
     - attribute targeting another model have limited validation:
        - the reason: they might be deleted|modified when the model they target is deleted, and that model deletion
          should not have to care about it, but in the same time we don't want invalid models
        - forbidden: required, minItems, dependencies, contains, custom validation, being part of
          $data cross-attributes validation
     - maybe instead:
        - if model being invalid after attribute removal, delete the model altogether???
  - find dead links and remove dead links:
     - should bypass authorization and pagination (using args.internal true)
     - actions should have no args.select, so they are not in output
     - make sure it does not impact "topLevel" action logic, since that logic might imply there is only one topLevel action
     - do not merge their "results" with other "results", to avoid authorization issue
     - after action performed (i.e. after db query for "remove dead links", and after 404 validation for "find dead links"),
       response.data should be [] to avoid extra computation
  - find dead links:
     - for create|patch|upsert actions
     - find all ids+modelName of attributes that:
        - have attr.target !== undefined
        - are defined in args.data
        - are a "leaf", e.g. in args.data { childA, childB: { ... } }, childA, not childB (since it will be created,
          it cannot be a dead link)
        - are not one of the models already being written, i.e. the id of one the models in:
           - (create|upsert) args.data
           - (patch) currentData results
     - timing:
        - create: run|started just before currentData middleware
        - patch|upsert:
           - run|started just after currentData middleware
           - only args.data that are being modified, i.e. defined in args.data and different from currentData results
        - create|patch|upsert: await the promise only just before write actions start, i.e. can be run in parallel
          of all middleware until then
     - goal: firing 404 if dead link
  - remove dead links:
     - for delete actions
     - silently skipped if target model's database adapter if does not have features "filter:eq|in|some"
     - timing:
        - run|started just after currentData middleware
        - await it, i.e. mergePatchData middleware must not start until this is done
     - first search for source models with dead links:
        - do find actions with args.filter { attr: { in: ids } } (isArray false) or { attr: { some: { in: ids } } }
          (isArray true) where ids are the models being deleted, on the models that might target them
        - group by source model type:
           - e.g. if two different model types have same parent model type but different parent model attribute,
             just use args.filter OBJ_ARR alternative
        - merge per source model attribute:
           - e.g. if two models have same parent model attribute, merge their ids in "in", removing duplicates
        - add args.filter { id: { neq: ids } } to exclude models already being deleted, i.e. already present in delete actions
          results
     - then create patch actions to remove dead links
        - do not fire those patch actions, only stack them up at the end, and resolveWriteAction will take care of them
     - should bypass 404, just in case the model got deleted in-between

Compatibility layer:
  - reporting deprecation
  - breaking changes:
     - notify when schema change introduces breaking change (e.g. graphql.js provides that)
  - think about how to version schema file
  - autoversioning:
     - might be related to breaking changes feature
  - migrations helpers
     - when changing schema constraints, should migrate data so they conform to new constraints
     - when adding default|value, should migrate data, otherwise when doing patch with empty data,
       or find followed by upsert, both should be idempotent, but default|value would actually be applied
  - should it generate redirects when name has changed?
  - feature flags

Versioning/changes:
  - can probably link together versioning, changes and undelete features
  - versioning:
     - on any model modification
     - should allow restoring
        - including undeleting:
           - use query parameter "show_deleted" and model attribute "deleted"
     - should allow searching
     - maybe, instead of introducing new goals/commands, use special attributes to search or modify
  - listening for changes:
     - should emit change events, but not know how they are used, i.e. not know the subscriber side
        - alternative is to emit events on ApiServer eventemitter for local consumption.
          Less decoupling, but easier to implement and consume.
          Might even be able to do both, e.g. add a module that translate local events into remote events.
     - should allow listeners to catch up if they missed some events because of network problems
     - should allow listeners to target specific: model, attribute, value, condition on value (e.g. value < 5),
       request context, request user, etc.
       Does not mean emitter perform those checks, but that it allows them to be performed
     - make it easy to integrate with SaaS integrations, e.g. "use this service to send an email"
        - standardize/simplify the interface to make it easy to create integrations
  - separate from rest:
     - done after the request was handed back to client, i.e. no impact on performance
     - done in different database
        - could be single table with streams of changes
  - can problably use standard diff format, e.g. JSON patch or JSON merge patch.
    Could also store models in full
  - can limit max number of versioned models by time or by absolute number or a combination
     - need to make sure change listeners can still get all events without model being removed from their reach
  - maybe use HTTP memento
  - maybe use semantic links, e.g. Link: <URI>; rel="alternate|canonical|latest-version|working-copy|predecessor-version|
    successor-version|version-history" [S]
  - must version schema file format itself, and also apiEngine itself
  - undelete feature

Schema strictness/polymorphism:
  - think about additional properties (not specified in schema), and whether to allow them in input, and in output:
     - at the moment, they are allowed
     - if want looser schema, think of impact on:
        - validation: see related JSON schema keywords: prohibited, additionalProperties, patternProperties, patternRequired,
          propertyNames, additionalItems
        - GraphQL schema
        - patch operators type validation, i.e. OBJ.attribute|argument 'TYPE'_ARR
     - should it be allowed to specify an unknown ATTR in args.order, args.aggregate, etc.?
  - type object|object[]:
     - probably not a good idea because:
        - not taken into account in limits (max attribute size, max number of models, max attributes per model)
        - schemaless, e.g. no description, etc.
           - if users need dynamic properties, can add this feature to models without having to add "type object"
        - object JSON schema validation is not super simple
        - denormalize where everything less in the engine is otherwise normalized
           - e.g. could not query those models, because not top-level
     - upside: less database queries
  - mixed arrays:
     - i.e. using schema.items SCHEMA_ARR instead of schema.items SCHEMA
     - also schema.additionalItems
     - re-verify error messages for those two properties
  - overloading, i.e. union types:
     - in GraphQL, use "union" maybe.
     - could be done by passing array of schema.type or schema.model
        - there is already some basic support for schema.type array in schema validation
  - subtyping:
     - both nominal (e.g. guessing type using an attribute as differentiator)
     - or structural (e.g. guessing type from which attributes are there, or which are their types)
     - in GraphQL, use "interface" maybe
     - add the moment GraphQL fragment "on TYPE" are noop, i.e. always return true whatever TYPE is

Enum:
  - having a better support for enum instead of just atte.validate.enum
  - potential benefits:
     - adding metadata (description, deprecation, etc.) on each value
        - can use GraphQL enum
     - smaller representation/size in database by using integer indexes, while client uses normal strings:
        - how to handle compatibility??? E.g. when removing an enum value, must migrate data???
    
New types:
  - beyond JSON ones
  - must be serializable|parsable from JSON:
     - i.e. should probably be serialized to string in network and traffic
  - possible types: date, regexp, function, binary, fixed point number
  - allow custom types
  - at API layer they are of the correct type:
     - in schema functions
     - different args.filter operators, including attr.authorize
     - different attr.validate keywords
  - must work with pagination cursor serialization
  - must work with GraphQL schema
  - can probably be used as model.id

Upgrade MongoDB doc:
  - upgrade doc of MongoDB, when it goes to 3.6 (see my doc for more info)

Async actions/tasks:
  - task management (restarting, retrieval, etc.)
  - Async actions must be well thought as they slow down requests:
     - there should be jobs reported to users and users should be able to control max wait time.
     - when this is figured out, think of how async schema functions would work within that
        - consider that paramsRef is directly mutated, i.e. might not be thread-safe
     - try to think if need generic async actions output (e.g. HTTP 202, Prefer: respond-async [C], Prefer: wait=NUM [C]
  - see REST doc for more info
  - hooks
     - reacting to attribute changes, or even just read, etc
     - fires JavaScript function
     - can also fire a module that follows specific interface, so it can easily
       be referred to
        - i.e. allows for microservices/addons ecosystem
        - should include a HTTP call addon, i.e. provides webhooks  

Alternate ids:
  - e.g. a query can either use machine-friendly `id` attribute, or human-friendly `name` attribute

Server routing:
  - compare with existing libraries, and see if should reuse one and/or their features

Realtime:
  - protocol-agnostic, i.e. WebSocket protocol is just one option
  - subscriptions (on-demand or automatic after a query|mutation)
  - use args.filter to targer specific models
  - target specific write actions, or any
  - info about old and new values
  - maybe something like:
     - create an endpoint to setup connection, where specify if want automatic or on-demand
     - if on-demand, must then pass extra parameter to request (via arguments)
       to specify want to subscribe

Concurrency conflicts:
  - locking or MVCC (automatic merge conflicts)
  - preconditions, including HTTP (e.g. If-Match [C])
  - errors, including HTTP 409
  - see concurrency chapter in to_learn.txt
  - problem with concept of "changed" in variants middleware:
      - e.g. if clientA fetches model as { a: 1 } then clientB saves it as { a: 2 }, then clientA saves it as { a: 2 } as well,
        "a" will not be considered "changed", although it should, because "changed" is not about which attributes changed from
        a server perspective, but about which attributes a client tried to change, i.e. from a client perspective.
      - e.g. variants middleware won't work properly, e.g. clientA fetches model as { oldA: 1, newA: 1 }, then clientB
        saves it as { oldA: 2, newA: 2 }, then clientA saves it as { oldA: 1, newA: 2 }.
        clientA intents here to chamge variant using "newA", but "oldA" will be used instead

Rate limiting:
  - should be shared between server instances
  - maybe at API gateway-level
  - see HTTP doc for standard headers and status codes

Security:
  - TLS
  - CORS
  - XSS
  - CSRF
  - general utilities, like "helmet"
  - should be protocol-agnostic as much as possible

Request timeout fix:
  - it currently uses Promise.race(), which does not cancel other promise when one fails,
    i.e. request keeps being processed even after timeout error response has been sent
      - instead, requestTimeout should only set a request-wide variable on setTimeout() callback.
        Before each middleware, this variable should be checked, and if set, throw an exception.
  - should call SERVER.setTimeout(NUM) too
  - requests that error should not have already modified the database, or should rollback
     - i.e. request timeout cannot happen after no rollback is possible anymore
     - do this by passing setTimeout() return value to mInput, and calling clearTimeout() after last command layer completed
  - think carefully of how much timeout should be:
     - including when parsing request bodies as big as the limit

Multiple databases:
  - allow using the same database adapter several times but with different options
     - how this is specified in the options is problematic:
        - using OBJ_ARR in options is not currently supported, and is creating many problems, so should avoid
        - using e.g. db.ADAPTER[_*].* notation for extra adapters is hard:
           - each adapter options should be parsed and validated the same in /src/options/
           - extra dynamic options should not appear in CLI --help message
           - CLI --help message should document that dynamic options can be added
           - CLI should be aware of dynamic options, so it parses them
           - maybe use YARGS.hidden()

Hidden attributes:
  - attr.hidden BOOL[_FUNC]:
  - on input, when true, behaves like attr.readonly
  - on output, unsets attribute
  - problems???
     - if schema is changed, and some clients that had hidden attribute now do not,
       any upsert from those clients with previously fetched models will erase the previously hidden attributes from database
     - impact on args.order_by|group_by|reduce|etc.???

HTTPS support

HTTP/2 support

Proxies:
  - make sure it works well with proxies

HTTP details:
  - 201 + Location [S]
  - HTTP server events "checkContinue" (for 100-continue [C]), "connect" (for CONNECT), "upgrade" (for Upgrade [C])
  - Accept-Patch [S]
  - Expect: 100-continue [C]

GraphQL:
  - problem with polymorphism in GraphQL schema:
     - nested attributes can either be a string or a nested object, in both selection and args.data
     - GraphQL does not allow union of scalar and object
     - also, GraphQL does not allow union types in input
     - this is problematic as GraphiQL:
        - shows queries as invalid
        - autocorrects queries, e.g. turning nested_model selection into nested_model { id }
     - same problem with args.filter, since args.filter.ATTR can be either VAL or a complex object
        - that complex object should depend on the type of VAL:
           - the leaves should be of the same type, e.g. { eq STR } if VAL is string
           - some operators are available only for specific types
        - should also not show operators if database does not support "filter:OPERATOR" feature
     - OBJ or OBJ_ARR:
        - can be either in:
           - args.data
           - response depending on *One or *Many
        - might be less of a problem since GraphQL spec allow VAL for [VAL]
  - fragments:
     - if we do not allow any kind of polymorphism, consider not allowing fragments, as they only make sense with polymorphism
     - fragment "on type" currently does not do anything
     - all fragments must be used
     - no recursive fragments
  - variables:
     - variable declarations does not currently do anything
     - no duplicate variables
     - no unused variables
  - introspection:
     - support __typename
     - support __type(name: 'TYPE')
     - support mixing GraphQL introspection query (e.g. __schema) with non-introspection query
     - maybe do this by reusing code from Graphql.js
  - go through GraphQL spec validation chapter again

Database transformation layer:
  - before database action middleware
  - e.g. one model in two tables, two models in one table, database-specific info, for both input|output
  - should allow single server to use multiple databases with different technologies (e.g. MongoDB + Redis) too
  - possibility: using variants.ATTR.ATTR2 (instead of variants.ATTR) with ATTR being a nested model
     - in that case, need to think about consequences on $model system variable

Custom commands:
  - GraphQL schema:
     - method: name, description
     - args: requiredness, type (including ARR), default value, description, deprecation reason
  - Schema file declaration
  - Global or model-wise???
  - Nesting???
  - allowing to call core commands???
  - rpc layer input???
  - GraphQL selection???
  - how to target command name with REST???
  - only allow POST method???
  - examples: API status, rate limiting status

Schema functions:
  - think if should use them in other parts of schema
  - Add system variables related to device|browser detection
     - add them to logging requestInfo too
  - make system variables immutable but:
     - not the user variables, cause they are external and might mutate
     - only once per system variable per request, because setting immutability is slow
        - what about $model???
  - using JSON references with plugins:
     - plugins can only inject inline functions, not JSON references:
        - because they cannot be sure of the relative path from the schema file to the injected JSON references
     - plugins can only use as options inline functions, not JSON references:
        - because JSON references are not compiled to actual JavaScript functions during plugin injection
           - e.g. author plugin opts.currentUser has to be inline, cannot be a JSON reference
        - maybe a solution would be to normalize any JSON reference passed as option to a plugin, to a user variable,
          and pass that user variable string to the plugin instead

Compile-time validation:
  - schema validation should not use JSON schema, but fastValidation to give better error messages
     - can probably remove ajv-keywords dependency after that
  - refactor validation to use single system. E.g. filter format validation uses a different one
  - should schema validate that validate.required|dependencies are only used at top-level:
     - because of JSON schema recursion, it's actually currently possible to do validate.allOf.required
     - this will crash, because we manipulate those properties at top-level, before calling ajv
  - according to each attr.type:
     - should validate that attr.validate only contain keywords for that type
     - attr.default is of that type
     - careful because attr.type is only decided after normalization
  - schema functions:
     - validates when can that take either schema function or a constant of the same type as the attribute,
       e.g. in attribute.default|value
     - make sure schema functions does not use unknown params, which is challenging because:
        - function body might not be readable, e.g. if function is bound
     - perform static analysis, e.g. linting
     - maybe validate complexity, e.g. max length

Custom code:
  - separating code into several packages
  - using a more plugin-oriented architecture:
     - allow users to write support for new protocols, rpcs, etc.
     - allow users to add|remove middleware
     - should do this with a decorated FUNC(APIENGINE)->APIENGINE, as opposed to using a run option
        - decorated APIENGINE should keep all features, including CLI
     - allow integrations (see above)
     - standardized how to do all this, e.g. with specific tooling, and with specific npm tags to easily list them

Callbacks/events:
  - not sure if this is a good idea
  - on new|finished layers
  - while events are async, callbacks are sync and allow modification of input/output

CLI:
  - dynamic options, such as db.mongodb.opts, does not work with CLI

systemInfo event payload:
  - add info about:
     - databases|protocols|formats|rpcs and their options
     - databases stats (e.g. dataset size, memory, load, etc.)

RegExp compatibility:
  - figure out common subset of RegExp flags and features supported by all database adapters,
    so that it is the same across database adapters

Database retries:
  - how to handle network or database connection failure???
  - e.g. if query fails because database connection is down, should try exponential retry???
  - done by API layer or by database adapter or by database library???

Server-client state:
  - $cookie:
     - user variable automatically set
     - only with HTTP
     - allow each protocol to do similar things (setting user variables)
     - check session middleware doc to get more inspiration
  - session:
     - retrieving a session object using a key-value store and an "id" specified as a schema function
     - slow down every request, so consider performance impact
    
Views:
  - are resources working as alias for a query:
     - e.g. GET /my_view is translated to GET /my_models?group=attr&filter...
     - allow overriding or adding args, e.g. GET /my_view/ID or GET /my_view?select=...
  - only for find action
  - might be more actively cached

Code quality:
  - auto-beautifier
  - escomplex

Caching:
  - protocol-level caching, including HTTP caching
     - allow specifying with protocol-agnostic settings, but also accept|produce standard HTTP caching
       (see Express.js for example)
  - automatic request caching, and invalidation:
     - between API and database
     - between client library and API
     - between client and client library, e.g. creating a client library that gets push from server on invalidation,
       so it does not even perform any request
     - saved on key-value store, so can be shared between instances
  - delta encoding

Optimization:
  - size of array returned by *Many actions should not impact too much response time:
     - e.g. at the moment many things (e.g. attr.value) is run once per model, where it could instead be run once for
       all models
  - look for memory leaks
     - check for memory leaks in memoize(), i.e. new requests should not increase memoize() memory retention
  - do performance profiling to see which parts are slow
  - memoize schema functions run per request. Instead of stringifying ifv, use === comparison
  - concatenate schema functions together:
     - apply schema functions in batch, i.e. instead of applying same attr.value to several models of same collection,
       transform schema function to $val.map(FUNC) and apply on collection instead
  - maybe use perf_hooks module instead of process.hrtime() + OBJ.measures for perf monitoring:
     - not sure if this is a good idea:
        - timerify() does not wait for async functions
        - FUNC.name is messed up by timerify()
        - 'mark' and 'measure' must be cleaned, which can be problematic if error happens in the middle of request middleware
        - mark() + measure() + getEntries() is a bit verbose and slow
        - 'make' and 'measure' names must be specific for each request, to not mix each request perf
     - on the flip side, this avoid explicit shared variable OBJ.measures
     - performance.nodeTiming for startup perf does not work, because it is as a library, so it would encompass the calling code
  - use streaming:
     - on compression|decompression and on charset decoding of request|response payloads
     - allow trailing headers

Limits:
  - when project is more stable, need to reconsider each limit:
     - to limit the hassle it is for end-user. E.g. pagination size too slow means paying for several requests, and longer overall request, which is frustrating.
     - while still having an ok processing time for average big requests
     - consider the limits of the DBaaS, and of the database they use, to avoid hitting those limits

Testing:
  - unit tests:
     - test coverage
     - data-driven tests
     - fuzz testing
  - integrated tests
  - load testing
  - performance testing
  - greenkeeper

CI/CD

Logging/monitoring:
  - logging:
     - dashboard
     - go through docs and to_learn
     - separate into distinct modules functions that take engine event payload format and
       transport to a specific place. I.e. act like logging plugins.
  - monitoring:
     - host metrics
     - alerting
  - health monitoring
     - status page
     - health/ping endpoint
     - think of added costs in a FaaS setup, and repercursion on pricing
        - e.g. pinging every second would cost 100ms task per second
  - distributed request tracing
  - analytics:
     - tell which models are used, which attributes, which versions, which params, etc.
     - might be able to build on logging feature ("call" type) for that

Dependencies:
  - package.json linting
  - learn more about npm, yarn, etc.
  - deprecation/security automatic check
  - changelog for the engine project itself
  - dependencies upgrades: choose strategy and tools (like greenkeeper)
  - deprecation for the features of the engine project itself

Less code:
  - replace some code by libraries:
     - /src/utilities/functional/
     - other /src/utilities/
     - /src/json_validation/

ES modules:
  - use ES6 import/export, when supported natively by Node.js without --experimental-modules, import() and import.meta supported
     - rename *.js to *.mjs
     - use import.meta instead of __dirname|__filename
     - use import, export and import()
     - fix ESLint rules for that
     - no need for 'use strict' anymore nor ESLint impliedStrict true
     - use shrimpit

Nodemon exit in production mode:
  - when server.keepAliveTimeout is left to its default value (i.e. 5000), and Nodemon is running, and a request
    has just been fired (i.e. socket is still alive because of timeout), hitting CTRL-C will fail at freeing the
    socket, i.e. restarting right after will fail.
  - a former solution I had was to fire process.kill(process.pid, 'SIGUSR2') on shutdown event, but this was problematic:
     - if several servers are run at once (with or without Nodemon), this will make the first one that finished exiting
       abrupt the others
     - it adds Nodemon-specific code

Live database:
  - DBaaS
  - backups
  - high-evailability
  - scalability

DevOps:
  - PaaS/FaaS
  - serverless:
     - since AWS lambda does not reuse Node REQ|RES, possible solutions:
        - treat AWS lambda as a different protocol, alongside HTTP
           - problem: there might be code duplication for the HTTP-related code, e.g. query string parsing
        - create utility that converts AWS lambda input to REQ/RES
  - easy to spawn multiple environments (stage, A/B testing, etc.)
  - Docker container
  - canary
  - rolling releases

System routing:
  - maybe as API gateway
  - load balancing
  - autoscaling

Authentication

Client:
  - Make some parts isomorphic, e.g. data validation, schema file loading, schema validation, etc.
  - Integration with frontend frameworks, client auto-generation

GraphQL relay:
  - must add clientMutationId, see https://facebook.github.io/relay/graphql/mutations.htm
  - must follow https://facebook.github.io/relay/graphql/objectidentification.htm

CLI tool:
  - for doing both administration, schema edition, or custom functions

Admin dashboard:
  - like Mr.Wolf, but automated
  - for content management, basically a GUI to the API

Debugging:
  - use GraphQL voyager instead of GraphiQL
  - graphiql should be according to Accept [C] (not route) with potential override
    with query variable like 'raw' to see raw result
  - HTML interactive output format when requesting from a browser

User documentation:
  - interactive examples
  - ability change examples protocol, interface, schema format and programming language

API documentation:
  - description:
     - build it using not only schema.description, but also schema.examples, schema.title and schema.* related to validation.
     - should be done during schema compile-time transformation
  - printSchema():
     - better sorting
     - maybe change endpoint or way to get there.
     - improve syntax highlighting.
     - also maybe offer option to show full version, and offer simplified version by default,
       e.g. showing only one action, and not showing variants (nested, singular|plural, etc.)
  - API auto-documentation:
     - see REST doc for idea of everything that can be documented
     - provide API console for experimentation
     - code examples
  - changelog generation
  - add error_uri URL in error messages, pointing to documentation
  - parse comments in schema file to include them in documentation and changelog.
    E.g. good to describe business-specific schema functions.
  - thing of what's not know at compile-time, e.g. limits

Thorough dev documentation:
  - use jsdoc, esdoc or similar
     - see ESLint rules
  - API engine documentation website

Fake server/endpoints:
  - fake data generation (using schema to guess type/constraints), including mixed with real data
  - easy mock server generation for client, by using schema file
  - could either be specified by client (e.g. arg) or server (e.g. in schema)

Meta-information:
  - schema retrieval:
     - through API, e.g. /MODEL/schema
     - validation:
        - model's validation JSON schema:
           - Content-Type: application/schema+json [S]
           - can be directly usable with a library like AJV
        - model's schema:
           - should also be linked to by each response as Link: <URI>; rel="describedby" [S]
  - semantic web
  - HATEOAS:
     - see REST documentation for ideas
  - general API "home document":
     - could use OPTIONS with HTTP as well

Other database adapters:
  - add support for other databases
  - each adapter:
     - should only throw DB_ERROR
     - should not throw when deleting an id that do not exist
     - potentially ADAPTER.id.name and ADAPTER.id.default()
        - prefer UUIDs so it looks consistent across databases
     - ADAPTER.type|title|description
     - ADAPTER.features:
        - and whether they should be checked startup-time (/src/run/database/) or query-time (validateFeatures middleware)
     - ADAPTER.connect|disconnect()
     - ADAPTER.query()
     - using undefined|null in both read and write
     - like REGEXP test should not be anchored, unless ^$ is used
     - should try to use the same option names accross adapters, e.g. port, host, username, password
     - should reconnect if connection is lost
        - should try to reconnect several times, but should give up quickly if does not work
        - devOps should be used instead to restart the machine

Other RPCs:
  - implement other RPCs, beyond GraphQL, REST and JSON-RPC

Other formats:
  - CSV
  - TSV
  - DSV
  - MSON
  - XML
  - protobuf
  - others
    
Other compression algorithms:
  - lzma
  - sdch
  - xz
  - bzip2
  - exi
    
Output format:
  - option to prettify output:
     - agnostic to output format.
     - should be as featureful as my JSON viewer Chrome extension: highligting, lines folding|collapsing, auto-URL-linker,
       toggle button to show raw, data available in console
     - automatically on when requesting from a browser.

Node.js version:
  - allow using other Node.js version than the latest

Other programming languages:
  - specified by using top-level property "language" in schema file
  - allow other programming languages in schema functions
  - allow other programming languages in functions imported by $ref
  - each new programming language must reimplement common functions like underscore.string
  - could also use transpilers, e.g. for TypeScript, etc.
  - make sure JavaScript-specific logic does not apply to other languages

Schema format:
  - convert Swagger|RAML|API blueprint conf files into schema format
     - allows those conf files as input, converting them first
  - yamllint the schema meta-schema
  - create proper schema file linter, inspired by ESLint
     - standard output (both as a string and as OBJ_ARR)
     - error locations
     - errors documentation URLS
     - autocorrections

Offline-first:
  - how:
     - do everything the server would do, but client-side
        - i.e. client must know schema
     - no need for server interaction, except for sync
        - this where server validates that client-side logic happened correctly
        - also where new data is available for other clients
           - which include realtime and conflicts
  - check couchDB and related for ideas about this
  - what about static assets???
     - some offline-first tools choose to bypass backend and let client directly interact with cloud provider

i18n:
  - language content negotiation
  - error messages

Privacy feature

Central BaaS API:
  - deploying backends using schema
  - schema file's user management (who can modify schema)
  - must submit not only schema but also directory/project around it:
     - because compiled schema still references files, including node modules
     - problem is size of hosting those directories
     - maybe should require GitHub repos, so no need to host

Sysadmin client app:
  - schema edition:
     - should perform client-side:
        - basic format validation, e.g. YAML linting following by YAML parsing
        - schema validation
        - schema test compilation
        - all this should reuse isomorphic server code
  - GUI to central BaaS API

Promotion:
  - commercial website
  - ads

Schema migrations:
  - guessing an schema from existing database.

Positioning:
  - main keywords: BaaS, featureful, easy, generic, stable, open-source
  - market: BaaS
  - main value: backend that is both featureful and easy to maintain
  - target audience:
     - developers, not newbyes
     - no assumptions on particular technologies or business cases
  - main requirements, in order:
     - easy:
        - maintainability: maintaining, setting up, upgrading, integrating, extending
        - manageability: operating, deploying, scaling, monitoring
        - learnability: documentation, support
        - UI dashboard: good UX, design, usability
     - stable:
        - tested, secure, reliable, available, recoverable
     - featureful:
        - any feature a backend can provide
        - high quality design/implementation of each feature
     - generic/agnostic:
        - prefer generic over specific, even it lowers efficiency or performance:
           - i.e. interoperability with specific tools (client libraries, databases, etc.) is not paramount
             although nice to have
        - flexibility:
           - allow customizing business logic, with least assumptions about it
           - do not allow end-users customizing API design:
              - prefer forcing good API design over flexibility
              - but encourage contributors to customize API design through generic plugin architecture
  - configuration:
     - featureful, i.e. many configuration options, which is ok
     - but easiness achieved thanks to:
        - minimal API surface for each option, by sacrificing specificity/efficiency over genericity
        - each option should have good default so they rarely need to be used
   - open source:
      - i.e. no vendor lock-in

 25  MUST HAVE FEATURES
 2   Pagination
 3   Config

 3   Errors
 1   inputValidation
 0.5 JSON schema $data

 4   Aggregation

 4   Static assets

 2   Orphans
?7   Compatibility layer
?7   Versioning/changes

 10  DATA MODEL
?5   Schema strictness/polymorphism
?1   Enum
?2   New types
 2   Upgrade MongoDB doc

 15  NICE TO HAVE FEATURES
?3   Async actions/tasks
?1   Alternate ids
?0.5 Server routing
?4   Realtime
?3   Concurrency conflicts
 1   Rate limiting
 3   Security
 0.5 Request timeout fix
 0.5 Multiple databases
 1   Hidden attributes

 15  NOT ESSENTIAL FEATURES
 2   HTTPS
 2   HTTP/2
 0.5 Proxies
 0.5 HTTP details
 2   GraphQL
?3   Database transformation layer
?2   Custom commands
 1   Schema functions
 2   Compile-time validation
?3   Custom code
?1   Callbacks/events
 0.5 CLI
 1   systemInfo
 0.5 RegExp compatibility
 0.5 Database retries
 2   Server-client state
 1   Views

 55  SOFTWARE QUALITY
 1   code quality
?5   Caching
?5   Optimization
 0.5 Limits
 30  Testing
 2   CI/CD
 6   Logging/monitoring
 2   Dependencies
 2   Less code
 1   ES modules
 0.1 Nodemon exit

 70  DEVOPS/LIVE
 2   Live database
?10  DevOps
?2   System routing
?20  Authentication
?10  Client
 2   GraphQL relay
?5   CLI tool
?25  Admin dashboard

 40  DEV FEATURES
 3   Debugging
 5   User documentation
 10  API documentation
 20  Thorough dev documentation
?2   Fake server
?3   Meta-information

 20  NOT IMPORTANT FEATURES
 *   Other database adapters
 *   Other RPCs
 *   Other formats
 *   Other compression algorithm
 1   Output format
 3   Node.js version
 10  Other programming languages
?    Schema format

?    Offline-first
?    i18n
?    Privacy feature
?    Central BaaS API
?    Sysadmin client app
?    Promotion
?    Schema migrations
?    Positioning
