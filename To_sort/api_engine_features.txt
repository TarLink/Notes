Dependencies:
  - upgrade yargs to 9.1.0 (when they push to npm)

Database:
  - ORM:
     - add ORM:
        - parse arg.filter as ORM object (see arg.filter normalization)
        - equality must be deep equality for ARR|OBJ, so people can do { filter: { attr: ARR|OBJ } }
        - should throw DATABASE_NOT_FOUND and DATABASE_MODEL_CONFLICT errors
     - fake ORMs:
        - in-memory: use pSetTimeout(0) to emulate async
        - in-filesystem: as JSON file
        - both should be atomic, i.e. all models in a *Many commands should be written at once
     - each ORM should have a specific "defaultId" function, used in defaultId middleware:
        - try to use UUIDs for most ORMs, so it looks consistent
  - arg.filter normalization:
     - small normalization done during normalization middleware:
        - does not change format, just add defaults, e.g. VAR: VAL becomes VAR: { eq: VAL }
        - also validate that arg.filter.id is defined and is the only one, when command findOne|deleteOne
     - format:
        - is to be determined??? But basically MongoDB/Waterline-like, client-friendly
        - is converted to actual database object at the database layer
        - should include "or" (alternatives), e.g. with ARR
        - "and" is just using several OBJ fields
        - "not" should probably not be top-level, i.e. { KEY: { not: ... } instead of { not: { KEY: ... }
           - reason: easier to validate unknown attrs in args.filter if all KEY are top-level
        - normalize it so that it's easier to write database adapters, and is easier to manipulate in middleware too
        - should at least include: in, nin, startsWith, endsWith, contains
     - think if some dataValidation makes sense for args.filter, e.g. minimum|maximum or type
     - fix GraphQL schema
     - some adapters might support smaller subset of args.filter format
        - i.e. can add some quite advanced features, like RegExp, without fears of compatibility problems
  - per-model database
  
Complex patches:
  - provide MongoDB-like patch syntax for patch command, including array manipulation

Orphans:
  - validation:
     - attribute targeting another model have limited validation:
        - the reason: they might be deleted|modified when the model they target is deleted, and that model deletion should not have to care about it, but in the same time we don't want invalid models
        - forbidden: required, minItems, dependencies, contains, custom validation, being part of $data cross-attributes validation
  - find dead links:
     - for create|patch|replace commands
     - find all ids+modelName that:
        - have attr.target !== undefined
        - are being modified, i.e. defined in args.data and different from currentData
        - are a "leaf", e.g. in args.data { childA, childB: { ... } }, childA, not childB (since it will be created, it cannot be a dead link)
     - run in parallel of currentData middleware:
        - how can it be in parallel since it needs currentData???
        - merged to same output, i.e. to currentData
        - should reuse each other (results and pendingPromises)
        - start at same time. end when both are over.
     - goal: firing 404 if dead link
        - the extra currentData is just an added bonus to speed up next middleware
  - remove dead links:
     - middleware fired after currentData middleware, before mergePatchData middleware
     - on delete commands
     - first search for models with dead links:
        - do find commands with args.filter { attr: { containsOrIs: ids } } where ids are the models being deleted, on the models that might target them
           - if the model being deleted contains bidirectional links, can optimize with faster command (i.e. filter directly by id) because the models ids are known
        - add args.filter { id: { $ne: ids } } to exclude models already being deleted, i.e. already present in delete actions currentData
     - merge the find commands results to currentData
     - then create patch actions to remove dead links
        - do not fire those patch actions, only stack them up at the end, and resolveWriteAction will take care of them
        - those actions should bypass authorization, i.e. be "internal"
  - bidirectional links:
     - attr.reverse "attr2"
        - IDL validate (before normalization)
           - forbid attr.reverse if attr.target is not defined
        - IDL pre-normalization:
           - attr2.reverse "attr" is added, unless already present. I.e. link is truly the same on both ends.
        - IDL validation (after normalization):
           - model2.attr2 must exist, and have attr2.target defined
           - attr.target and attr2.target must be defined and point at each others models
           - attr.reverse "attr2" and attr2.reverse "attr" must be defined and point at each other (using also attr[2].target)
     - middleware:
        - on create|patch|replace commands
        - after currentData middleware, before removeDeadLinks middleware
        - fired for any attribute:
           - with attr.reverse defined
           - and that is being modified, i.e. defined in args.data and different from currentData
        - first merges into currentData some find commands results:
           - all ids being modified should trigger find commands
              - except if alread present in currentData. This should cover all ids being addes, since findDeadLinks middleare already searched for them.
        - then create patch actions to modify all the reverse ids
           - both the ones being added, and the ones being removed
           - do not fire those patch actions, only stack them up at the end, and resolveWriteAction will take care of them
           - those actions should bypass authorization, i.e. be "internal"

Atomicity/rollbacks:
  - in resolveWriteActions(), replace Promise.all() by something that waits for all promises to complete,
    even on failure, and return all of them
  - add rollbackInfoBefore and rollbackInfoAfter middleware just before and after databaseExecute,
    which uses a variable "rollbackInfo" shared by all write commands
  - if any command throws, set rollbackInfo.hasFailed true
  - in rollbackInfoBefore middleware, if rollbackInfo.hasFailed true, throw error
  - in rollbackInfoAfter middleware, do rollbackInfo.completedCommands.push(modelName)
  - resolveWriteActions() waits for completion of all commands, failure or not. Then, if any failed, it:
       - does a rollback on each rollbackInfo.completedCommands
       - wait for rollback completion
       - rethrow initial error
       - if rollback itself threw an error, set rollback error stack trace to initial error.failedRollback STR,
         using error.extra
  - rollback reverse the command, i.e.:
       - create -> delete
       - patch|replace -> replace with currentData
       - delete -> create with currentData
    
REST:
  - including:
     - selecting
        - including nested selections, to mimic GraphQL
        - reuse the "all" special attribute
     - populating
     - right status codes, e.g. 201 + Location [S]
     - Accept-Patch [S]
     - return value aliasing, like in GraphQL, by using args.select "ATTR=ALIAS" (already implemented)
     - same output as GraphQL, except errors OBJ_ARR is error OBJ
  - use user-friendly format:
     - think of URL encoding, and avoid chars that should be eacaped
     - can use ?ARG instead of ?ARG=true
     - automatically transtype value, using the transtype utility
  - fix user documentation to make it more operation-agnostic

JSON-RPC:
  - add error code number to error reasons
  - no batch requests
  - both versions 1.0 and 2.0. Same endpoint, but use version field to distinguish.
  
Operations:
  - implement other operations, beyond GraphQL, REST and JSON-RPC

Pagination:
  - problems (should document them in code):
     - parent actions are performed several times while going through the child action pages:
        - this is inefficient
        - this creates duplicates results, i.e. clients need to remove them
        - does not work when parent action cannot be performed twice, e.g. delete or create
     - this creates one pagination output metadata per action, which is more complex
  - solution: max models limit:
     - top-level actions are paginated, i.e. limited by runOpts.pageSize
     - nested actions are not paginated. Instead, two limits:
        - maxModels:
           - each operation can contain maximum NUM models, including top-level and nested actions. This is operation-wise, not action-wise.
              - does not include duplicate models. E.g. if fetching two nested models with same id, consistency middleware will remove duplicates, so they will not impact database load, and should not be counted in limit.
           - NUM is runOpts.maxModels.
              - defaults to 100 * runOpts.pageSize (not args.pageSize)
           - for many-to-many nested operations that hit that limit, two solutions:
              - decreasing args.pageSize
              - split the nested operation into several flatter operations
        - each nested action OBJ_ARR output length must be maximum pageSize:
           - this is to avoid using nesting to circumvent pagination
           - should count duplicates, not ignore them
           - it is for each OBJ_ARR, not whole nested action. E.g. if a parent has 100 OBJ_ARR children, each child should be under that limit,
             even though they will all trigger one single action, which might return more than that limit
     - for both limits:
        - if over limit, throws error
           - should trigger rollback
           - if it can be thrown before write actions are fired, it is even better as it avoids rollback
        - should use database offset|limiting NUM+1 in child actions to limit database load
        - for all commands, including createMany and replaceMany
  - general (top-level actions):
     - *One commands: no pagination
     - createMany, replaceMany:
        - no pagination args nor output metadata
        - but if args.data length (top-level only) > runOpts.pageSize, throws error
        - do not allow args.page_size
     - findMany:
        - either:
           - offset vs cursor:
              - is offset pagination if args.page !== undefined
              - default is cursor pagination otherwise, with args.after ""
              - document difference with offset pagination: not random-access, but stronger consistency/isolation
           - offset pagination:
              - args:
                 - page, page_size: same as current system
              - metadata.pagination:
                 - page, page_size: same as current system
                 - has_previous_page, has_next_page: same as current system
                 - total_size NUM, page_count NUM:
                    - calculated by doing an extra count database query
                    - always performed, but on offset pagination only
           - cursor pagination:
              - args:
                 - page_size: same as current system
                 - after, before: same as current system:
                    - including:
                       - how cursors are calculated
                       - "" cursors for beginning and end
              - metadata.pagination:
                 - page_size: same as current system
                 - has_previous_page, has_next_page: same as current system
                 - previous|next_token "CURSOR":
                    - instead of current system where each model has its own token
                    - those two cursors are the cursor calculated of the last and first model
                    - undefined if no previous or no next page
                    - cursors should store oArgs and use them for comparisons, not normalized args.
                       - clients should submit the same args.filter|order_by (instead of not submitting any, as it is now), i.e. only add args.after|before.
                 - first_token|last_token: always ""
     - patchMany, deleteMany:
        - like findMany except for the following points
        - no offset pagination
        - only forward cursors, i.e. no args.before nor  metadata.pagination.previous_token|first_token|last_token. metadata.pagination.has_previous_page is always false.
        - paginates the write database query itself:
           - i.e. cannot patch nor delete too many models at once
           - do it by paginating first internal find command, then use the ids in replace|patch|delete command
              - i.e. replace|patch|delete commands do not use pagination middleware for cursor pagination directly
  - middleware:
     - handles all types of pagination: max args.data length, cursor pagination, offset pagination.
     - e.g. max args.data length not done during args syntax check middleware
  - runOpts.pageSize:
     - instead of the three runOpts currently used, i.e. maxPageSize, defaultPageSize and maxDataLength
     - defaulting to 100
     - args.page_size can be used to change pageSize:
        - but can only be lower
        - integer, minimum 1
        - with findMany, patchMany, deleteMany
     - 0 to disable pagination:
        - i.e. no paginating, pagination args nor output metadata
        - args.page_size can be anything
  - metadata:
     - should not be selectable, i.e. always fully rendered in output
     - operation-wise, i.e. not action-wise, nor model-wise
     - in output, is sibling to "data", in both GraphQL and REST, i.e. { data OBJ, metadata OBJ }
     - should be undefined if args.silent true
     - go through the whole codebase looking for "metadata" because there's been quite some changes
  - relay.js:
     - requires model-wise cursors, which is too slow, and too complicated for users, i.e.:
       - only expose Relay.js cursors as opt-in
       - only first and last cursor are calculated. The others simply append an offset to the first cursor.
          - it is not as isolation-safe, but is much faster and only meant for Relay.js
  - protocol-level:
     - use Range [C] and Accept-Ranges [S] for pagination
        - e.g. transforming them into pagination regular arguments
     - first|last|previous|next:
        - like first|last|previous|next_token, but prepended with URL to perform the query
        - for GraphQL, need to put query in the URL query string
        - also as HTTP Link: <URL>; rel="first|last|prev|next|self[ ...]"

Authorization:
  - IDL functions for idl|model|attr.authorize:
     - FUNC[_ARR]: ARR is && (normalized during IDL transformation)
     - can use IDL functions variables|helpers
  - command "patch" cannot be user-specified. Instead it checks both "find" and "replace" permissions
  - idl.authorize FUNC[_ARR]:
     - for all models, performed once per request, just before main operation middleware
     - i.e. less IDL functions variables, and no partial evaluation
     - if resolves to false, do not perform query and returns 403
  - model.authorize FUNC[_ARR]:
     - model-wise
     - if fails, returns 403 (except for find|delete commands which might augment arg.filter)
     - according to command:
        - find|delete: $/$$: augment arg.filter (see below)
        - create: $/$$: use arg.data
        - replace: $/$$: use both arg.data and args.currentData (tried after another)
        - replace|create: if command.multiple, check model.authorize for each arg.data and each args.currentData.
          If any fails, 403.
     - in authorization middleware, just before dataValidation middleware
     - find|delete commands use partial evaluation:
        - exception during partial evaluation should throw 5** not 4**
        - use resulting database query object, if there is one, by && it
           - if resolves to false, do not perform query and returns 403
        - must try partial evaluation during IDL validation to make sure it works runtime
        - do not pass $/$$ to non-inline functions, because cannot be partially evaluated
     - find commands of replace command should not be affected (using args.authorization false)
  - attr.authorize FUNC[_ARR]:
     - same as model.authorize, except:
        - can use $ (same as $$.ATTR)
        - command when not authorized differs. Depending on command:
           - find:
              - silently removed in output
              - only checked if defined in output (i.e. key is present)
              - on all commands
           - create|replace|delete:
              - silently replace new value (arg.newData.ATTR) by current value (args.currentData.ATTR)
              - only checked if new value !== current value
              - in order: if current value is empty, use "create". If new value is empty, use "delete".
                Otherwise, use "replace".
              - on replace value only???
  - model|attr.authorize.COMMANDS:
     - like authorize, but applied according to current command
     - authorize VAL is normalized (by IDL transformation) to authorize.all VAL
     - COMMANDS can be find|replace|create|delete|all. Can be comma-separated list.
        - command.multiple does not matter, i.e. no authorization mismatch possible on nested commands
     - replaces idl.commands and model.commands (which must be removed):
        - forbidden commands:
           - are commands whose authorize always return false for a
             given set of protocol/operation IDL functions variables + $COMMAND param (e.g. without $/$$)
              - how to know? partial evaluation?
           - 403 becomes 405 on forbidden commands
           - forbidden commands not shown in GraphQL schema
        - add Allow [S] on every response:
           - does not show forbidden commands

Aggregating:
  - are just other commands, following similar logic as the others
     - should also think of which other commands it requires, and which one it implies
  - e.g. countUsers, groupUsers, etc.
  - arg.distinct "ATTR": no duplicates
  - is basically asking for server-side multi/single-ATTR/OBJ operations, so maybe try to allow client to specify
    it as JavaScript, parsed as MongoDB aggregate object???
  - think of interaction with pagination

Compatibility layer:
  - reporting deprecation
  - breaking changes:
     - notify when schema change introduces breaking change (e.g. graphql.js provides that)
  - think about how to version IDL file
  - autoversioning:
     - might be related to breaking changes feature
  - migrations helpers
     - when changing IDL constraints, should migrate data so they conform to new constraints
     - when adding default|transform, should migrate data, otherwise when doing patch with empty data,
       or find followed by replace, both should be idempotent, but default|transform would actually be applied
  - should it generate redirects when name has changed?
  - feature flags

Versioning/changes:
  - can probably link together versioning, changes and undelete features
  - versioning:
     - on any model modification
     - should allow restoring
        - including undeleting:
           - use query parameter "show_deleted" and model attribute "deleted"
     - should allow searching
     - maybe, instead of introducing new goals/commands, use special attributes to search or modify
  - listening for changes:
     - should emit change events, but not know how they are used, i.e. not know the subscriber side
        - alternative is to emit events on ApiServer eventemitter for local consumption.
          Less decoupling, but easier to implement and consume.
          Might even be able to do both, e.g. add a module that translate local events into remote events.
     - should allow listeners to catch up if they missed some events because of network problems
     - should allow listeners to target specific: model, attribute, value, condition on value (e.g. value < 5),
       request context, request user, etc.
       Does not mean emitter perform those checks, but that it allows them to be performed
     - make it easy to integrate with SaaS integrations, e.g. "use this service to send an email"
        - standardize/simplify the interface to make it easy to create integrations
  - separate from rest:
     - done after the request was handed back to client, i.e. no impact on performance
     - done in different database
        - could be single table with streams of changes
  - can problably use standard diff format, e.g. JSON patch or JSON merge patch.
    Could also store models in full
  - can limit max number of versioned models by time or by absolute number or a combination
     - need to make sure change listeners can still get all events without model being removed from their reach
  - maybe use HTTP memento
  - maybe use semantic links, e.g. Link: <URI>; rel="alternate|canonical|latest-version|working-copy|predecessor-version|
    successor-version|version-history" [S]
  - must version IDL file format itself, and also apiEngine itself

Static assets:
  - take inspiration from existing ones, probably reusing one
  - take inspiration from Express sendFile()
  - think of Content-Disposition
  - integrate GraphiQL with this
  - server-side templates serving
     - including isomorphic server-side renderer

Exception handling:
  - error reasons:
     - rationalize|reduce error reasons:
        - should be not about guessing where error was internally triggered, but in terms of how client should respond.
        - e.g. it does not matter if InputValidation error is because args.data is wrong or args.filter, what matters is that
          client displays that input is wrong.
  - should add `extra` information to any throw ERROR that possess interesting information.
     - all errors with the same `reason` should expose the same `extra` variables
  - document available error responses types, error `extra` and status codes
  - add all `generic.title`

Error reporting:
  - improve error messages for JSON schema composed types: schema.contains|propertyNames|not|anyOf|oneOf|if|then|else (others are fine)
  - error response should include parsable info for validation errors:
     - data that failed
     - data path: either as JSON path (def) or as JSON pointer (using a runOpt)
     - rule (e.g. 'exclusiveMinimum')
     - operation context (e.g. command, command_path)
     - any other relevant info
  - add `locations` attribute to GraphQL errors, and possibly other documented in my GraphQL doc
  - autocorrection|suggestions:
     - e.g. wrong|mispelled command|attribute name. Should be operation-independent
  - variants: if data validation fails because of an attribute that was generated as a variant, error message must include information that it was generated as such, otherwise it's confusing for end-user
  - request timeout error includes very little information (e.g. does not include protocol) because it is fired in
    an early middleware

JSON schema $data:
  - $data notation is a bit cryptic and prone to error with JSON pointers.
     - should replace to something more user-friendly, where user just need to specify the sibling model's attribute name
     - at the moment, $data is removed before IDL validation, this would need to be changed
     - validateMap would need to convert this notation to $data, for ajv to work
     - error reporting does not currently work with $data, because ajv does not translate $data into the actual referred
       data. E.g. it says "should equal [Object object]" because { $data: STR } is kept as is

inputValidation middleware:
  - like dataValidation middleware but before any server-side data transformation:
     - i.e. beginning of command layer
     - reuse attr.validation like dataValidation middleware
        - reason we do not have two different sets of validation: data stored in database should be valid input, and also having two sets adds two much complexity and room for errors to end-users
     - gives 4** client-side errors, while dataValidation middleware gives 5** server-side errors
     - input only (like dataValidation middleware)
     - should try to memoize|reuse per request (to avoid memory leaks) between inputValidation and dataValidation middlewars

Data correctness:
  - code makes assumptions about database correctness:
     - e.g. that created_time is defined, no dead links, etc.
     - should list those assumptions
  - should have a way to check and report database incorrectness
     - maybe an instruction that checks for problems by performing find commands
     - might also have an autofix feature

Schema strictness/polymorphism:
  - think about additional properties (not specified in schema), and whether to allow them in input, and in output:
     - at the moment, they are allowed
     - if want looser schema, think of impact on:
        - validation: see related JSON schema keywords: prohibited, additionalProperties, patternProperties, patternRequired,
          propertyNames, additionalItems
        - GraphQL schema
  - should it be allowed to specify an unknown ATTR in order_by, reduce, etc.?
  - type object|object[]:
     - probably not a good idea because:
        - not taken into account in limits (max attribute size, max number of models, max attributes per model)
        - schemaless, e.g. no description, etc.
           - if users need dynamic properties, can add this feature to models without having to add "type object"
        - object JSON schema validation is not super simple
        - denormalize where everything less in the engine is otherwise normalized
           - e.g. could not query those models, because not top-level
     - upside: less database queries
  - allow mixed arrays:
     - i.e. using schema.items SCHEMA_ARR instead of schema.items SCHEMA
     - also schema.additionalItems
     - re-verify error messages for those two properties
  - overloading, i.e. union types:
     - in GraphQL, use "union" maybe.
     - could be done by passing array of schema.type or schema.model
        - there is already some basic support for schema.type array in IDL validation
  - subtyping:
     - both nominal (e.g. guessing type using an attribute as differentiator)
     - or structural (e.g. guessing type from which attributes are there, or which are their types)
     - in GraphQL, use "interface" maybe
     - add the moment GraphQL fragment "on TYPE" are noop, i.e. always return true whatever TYPE is

API base types:
  - attributes that are objects (or array of objects) but not models:
     - at the moment, are not allowed, because we want the user to take advantage of using models: creates endpoint,
       documentation, etc.
     - think whether this is a good idea?
     - this means JSON schema object validation will never be used (although technically allowed)
  - enum type:
     - with possibility to add metadata (description, deprecation, etc.) on each value
     - think if needs to allow transform between database representation and API representation
     - might use schema.constant|enum in JSON schema
     - might use GraphQL enum
  - new types beyond JSON ones:
     - must still be serializable|parsable from JSON
     - must be of the correct type when in API layer
     - must work with arg.filter normalization
     - must work with pagination cursor serialization
     - might want special GraphQL types for proper schema, e.g. special GraphQL scalar types
     - validation layer must handle them correctly
     - e.g. undefined, function, Infinity|NaN
     - allow custom types

Async actions/tasks:
  - task management (restarting, retrieval, etc.)
  - Async actions must be well thought as they slow down requests:
     - there should be jobs reported to users and users should be able to control max wait time.
     - when this is figured out, think of how async IDL functions would work within that
        - consider that paramsRef is directly mutated, i.e. might not be thread-safe
     - try to think if need generic async actions output (e.g. HTTP 202, Prefer: respond-async [C], Prefer: wait=NUM [C])
  - see REST doc for more info

Alternate ids:
  - e.g. a query can either use machine-friendly `id` attribute, or human-friendly `name` attribute

Server routing:
  - compare with existing libraries, and see if should reuse one and/or their features

Realtime:
  - protocol-agnostic, i.e. WebSocket protocol is just one option
  - subscriptions (on-demand or automatic after a query|mutation)
  - maybe something like:
     - create an endpoint to setup connection, where specify if want automatic or on-demand
     - if on-demand, must then pass extra parameter to operations (via arguments)
       to specify want to subscribe

Concurrency conflicts:
  - locking or MVCC (automatic merge conflicts)
  - preconditions, including HTTP (e.g. If-Match [C])
  - errors, including HTTP 409
  - see concurrency chapter in to_learn.txt
  - problem with concept of "changed" in variants middleware:
      - e.g. if clientA fetches model as { a: 1 } then clientB saves it as { a: 2 }, then clientA saves it as { a: 2 } as well,
        "a" will not be considered "changed", although it should, because "changed" is not about which attributes changed from
        a server perspective, but about which attributes a client tried to change, i.e. from a client perspective.
      - e.g. variants middleware won't work properly, e.g. clientA fetches model as { oldA: 1, newA: 1 }, then clientB
        saves it as { oldA: 2, newA: 2 }, then clientA saves it as { oldA: 1, newA: 2 }.
        clientA intents here to chamge variant using "newA", but "oldA" will be used instead

Rate limiting:
  - should be shared between server instances
  - maybe at API gateway-level
  - see HTTP doc for standard headers and status codes

Security:
  - TLS
  - CORS
  - XSS
  - CSRF
  - general utilities, like "helmet"
  - should be protocol-agnostic as much as possible

Request timeout fix:
  - it currently uses Promise.race(), which does not cancel other promise when one fails,
    i.e. request keeps being processed even after timeout error response has been sent
      - instead, requestTimeout should only set a request-wide variable on setTimeout() callback.
        Before each middleware, this variable should be checked, and if set, throw an exception.
  - requests that error should not have already modified the database, or should rollback
     - i.e. request timeout cannot happen after no rollback is possible anymore
     - do this by passing setTimeout() return value to mInput, and calling clearTimeout() after last command layer completed
  - think carefully of how much timeout should be:
     - including when parsing request bodies as big as the limit

Protocols:
  - HTTP server events "checkContinue" (for 100-continue [C]), "connect" (for CONNECT), "upgrade" (for Upgrade [C])
  - HTTP/2
  - make sure it works well with proxies

HTTP details:
  - Add support for 204 status code, with any write goal

GraphQL:
  - problem with polymorphism in GraphQL schema:
     - nested attributes can either be a string or a nested object, in both selection and args.data
     - GraphQL does not allow union of scalar and object
     - also, GraphQL does not allow union types in input
     - this is problematic as GraphiQL:
        - shows queries as invalid
        - autocorrects queries, e.g. turning nested_model selection into nested_model { id }
  - fragments:
     - if we do not allow any kind of polymorphism, consider not allowing fragments, as they only make sense with polymorphism
     - fragment "on type" currently does not do anything
     - all fragments must be used
     - no recursive fragments
  - variables:
     - variable declarations does not currently do anything
     - no duplicate variables
     - no unused variables
  - introspection:
     - support __typename
     - support __type(name: 'TYPE')
     - support mixing GraphQL introspection query (e.g. __schema) with non-introspection query
     - maybe do this by reusing code from Graphql.js
  - go through GraphQL spec validation chapter again

Create dryruns:
  - problem:
     - create* dryruns must fail if models already exist
     - database adapters should be agnostic to this, i.e. should just perform find queries to check for model existence
     - we want to avoid doing one query for each model, but still handle the situation where some models exist,
       while some others do not
  - solution:
     - create* dryruns transform to a find query (like delete dryruns)
        - add args.filter.id using args.newData. Remove args.filter as soon as database check finishes, so that next middleware
          do not use it
        - add args.createDryRun true so that databaseExecute middleware knows
     - a findMany query is performed:
        - if *all* model ids returned a 404, catch the error and return same response as replace dryrun
           - this requires database adapters to return *all* failing ids when throwing a 404, which I am unsure whether
             it is too much to ask???
        - if *any* model id does not return a 404, throw same error as when trying to create a model that already exist

Direct database access:
  - how to handle when modification is done directly on the database, not through the API layer?
     - example when this might create problems: no versioning done, no validation done, no transformation done, etc.
     - should direct access to database be readonly? Should there be a Cron task fixing data instead?

Database constraints:
  - uniqueness
  - indexes
  - non-null

Model naming:
  - idl.models.MODEL.names STR_ARR:
     - instead of MODEL.model
     - while MODEL is how model is refered to in IDL, and is stored in database, this is about how model is communicated to
       client (e.g. GraphQL command names or REST URLs)
        - attr.type must refer to MODEL, not MODEL.model, to avoid name conflicts with JSON type,
          i.e. so that models can be called "string", "array", etc.
     - def to ["MODEL"]
     - must be IDL validated
     - if ARR.length > 1, extra ones are aliases
        - when an alias was used, should communicate canonical name (i.e. ARR[0]) to client using header: X-Api-Engine-Model-Name: MODEL [S] (protocol-agnostic)
          and Content-Location: MODEL [S] (HTTP-specific)

Database transformation layer:
  - before database action middleware
  - e.g. one model in two tables, two models in one table, database-specific info, for both input|output
  - should allow single server to use multiple databases with different technologies (e.g. MongoDB + Redis) too
  - possibility: using variants.ATTR.ATTR2 (instead of variants.ATTR) with ATTR being a nested model
     - in that case, need to think about consequences on $$ IDL function variable

Custom commands:
  - must be declared in IDL file
  - are global, not model-wise? if model-wise, nesting?
  - think of nested actions
  - middleware at beginning of action layer
  - must contain:
     - handler function:
        - can call core commands with normal args
        - get input from operation layer
     - information to build its GraphQL schema, and do input|output validation

IDL functions:
  - think if should use them in other parts of IDL
  - Add variables related to device|browser detection
     - add them to logging requestInfo too
  - think of we can simplify idl.helpers IDL syntax
  - make IDL variables immutable but:
     - not the helpers, cause they are external and might mutate
     - only once per IDL variable per request, because setting immutability is slow
  - using JSON references with plugins:
     - plugins can only inject inline functions, not JSON references:
        - because they cannot be sure of the relative path from the IDL file to the injected JSON references
     - plugins can only use as options inline functions, not JSON references:
        - because JSON references are not compiled to actual JavaScript functions during plugin injection
           - e.g. author plugin opts.currentUser has to be inline, cannot be a JSON reference
        - maybe a solution would be to normalize any JSON reference passed as option to a plugin, to a helper,
          and pass that helper string to the plugin instead

Compile-time validation:
  - IDL validation should not use JSON schema, but fastValidation to give better error messages
  - should IDL validate that validate.required|dependencies are only used at top-level:
     - because of JSON schema recursion, it's actually currently possible to do validate.allOf.required
     - this will crash, because we manipulate those properties at top-level, before calling ajv
  - according to each attr.type:
     - should validate that attr.validate only contain keywords for that type
     - attr.default is of that type
     - careful because attr.type is only decided after normalization
  - IDL functions:
     - validates when can that take either IDL function or a constant of the same type as the attribute,
       e.g. in attribute.default|transform
     - make sure IDL functions does not use unknown params, which is challenging because:
        - function body might not be readable, e.g. if function is bound
     - perform static analysis, e.g. linting
     - maybe validate complexity, e.g. max length

Custom code:
  - separating code into several packages
  - using a more plugin-oriented architecture:
     - allow users to write support for new protocols, operations, etc.
     - allow users to add|remove middleware
     - should do this with a decorated FUNC(APIENGINE)->APIENGINE, as opposed to using a run option
        - decorated APIENGINE should keep all features, including CLI
     - allow integrations (see above)
     - standardized how to do all this, e.g. with specific tooling, and with specific npm tags to easily list them

Callbacks/events:
  - not sure if this is a good idea
  - on new|finished layers
  - while events are async, callbacks are sync and allow modification of input/output

Code quality:
  - auto-beautifier
  - escomplex

Caching:
  - protocol-level caching, including HTTP caching
     - allow specifying with protocol-agnostic settings, but also accept|produce standard HTTP caching
       (see Express.js for example)
  - automatic request caching, and invalidation:
     - between API and database
     - between client library and API
     - between client and client library, e.g. creating a client library that gets push from server on invalidation,
       so it does not even perform any request
     - saved on key-value store, so can be shared between instances
  - delta encoding

Optimization:
  - size of array returned by *Many commands should not impact too much response time:
     - e.g. at the moment many things (e.g. attr.transform) is run once per model, where it could instead be run once for
       all models
  - look for memory leaks
     - check for memory leaks in memoize(), i.e. new requests should not increase memoize() memory retention
  - do performance profiling to see which parts are slow
  - memoize IDL functions run per request. Instead of stringifying ifv, use === comparison
  - concatenate IDL functions together:
     - e.g. `test` in transform/value can be { test TEST, value VALUE } ->{ value (TEST ? VALUE : $) }
     - e.g. array of transform/value can be concatenated into a single IDL function
     - apply IDL functions in batch, i.e. instead of applying same transform to several models of same collection, transform
       IDL function to $.map(FUNC) and apply on collection instead

Limits:
  - when project is more stable, need to reconsider each limit:
     - to limit the hassle it is for end-user. E.g. pagination size too slow means paying for several requests, and longer overall request, which is frustrating.
     - while still having an ok processing time for average big requests
     - consider the limits of the DBaaS, and of the database they use, to avoid hitting those limits

Streaming:
  - input|output streaming:
     - protocol-level, e.g. HTTP
     - format-level, e.g. ndjson or YAML streams
  - Expect: 100-continue [C]

Testing:
  - unit tests:
     - test coverage
     - data-driven tests
     - fuzz testing
  - integrated tests
  - load testing
  - performance testing
  - greenkeeper

CI/CD

Logging/monitoring:
  - logging:
     - dashboard
     - go through docs and to_learn
  - monitoring:
     - host metrics
     - alerting
  - health monitoring
     - status page
     - health/ping endpoint
     - think of added costs in a FaaS setup, and repercursion on pricing
        - e.g. pinging every second would cost 100ms task per second
  - distributed request tracing
  - analytics:
     - tell which models are used, which attributes, which versions, which params, etc.
     - might be able to build on logging feature ("call" type) for that

Dependencies:
  - package.json linting
  - learn more about npm, yarn, etc.
  - deprecation/security automatic check
  - changelog for the engine project itself
  - dependencies upgrades: choose strategy and tools (like greenkeeper)
  - deprecation for the features of the engine project itself

Refactoring:
  - performance monitoring is very verbose in the code on startup and exit
  - use ES6 import/export, when supported natively by Node.js
     - fix ESLint rules for that
     - no need for 'use strict' anymore nor ESLint impliedStrict true
     - use shrimpit

Nodemon exit in production mode:
  - when server.keepAliveTimeout is left to its default value (i.e. 5000), and Nodemon is running, and a request
    has just been fired (i.e. socket is still alive because of timeout), hitting CTRL-C will fail at freeing the
    socket, i.e. restarting right after will fail.
  - a former solution I had was to fire process.kill(process.pid, 'SIGUSR2') on shutdown event, but this was problematic:
     - if several servers are run at once (with or without Nodemon), this will make the first one that finished exiting
       abrupt the others
     - it adds Nodemon-specific code

Live database:
  - DaaS
  - backups
  - high-evailability
  - scalability

DevOps:
  - PaaS/FaaS
  - serverless:
     - since AWS lambda does not reuse Node REQ|RES, possible solutions:
        - treat AWS lambda as a different protocol, alongside HTTP
           - problem: there might be code duplication for the HTTP-related code, e.g. query string parsing
        - create utility that converts AWS lambda input to REQ/RES
  - easy to spawn multiple environments (stage, A/B testing, etc.)
  - Docker container
  - canary
  - rolling releases

System routing:
  - maybe as API gateway
  - load balancing
  - autoscaling

Authentication

Client:
  - Make some parts isomorphic, e.g. data validation, IDL file loading, IDL validation, etc.
  - Integration with frontend frameworks, client auto-generation

GraphQL relay:
  - must add clientMutationId, see https://facebook.github.io/relay/graphql/mutations.htm
  - must follow https://facebook.github.io/relay/graphql/objectidentification.htm

CLI tool:
  - for doing both administration, IDL edition, or custom functions

Admin dashboard:
  - like Mr.Wolf, but automated
  - for content management, basically a GUI to the API

Debugging:
  - use GraphQL voyager instead of GraphiQL
  - graphiql should be according to Accept [C] (not route) with potential override
    with query variable like 'raw' to see raw result
  - HTML interactive output format when requesting from a browser

User documentation:
  - interactive examples
  - ability change examples protocol, interface, IDL format and programming language

API documentation:
  - description:
     - build it using not only schema.description, but also schema.examples, schema.title and schema.* related to validation.
     - should be done during IDL compile-time transformation
  - printSchema():
     - better sorting
     - maybe change endpoint or way to get there.
     - improve syntax highlighting.
     - also maybe offer option to show full version, and offer simplified version by default,
       e.g. showing only one command, and not showing variants (nested, singular|plural, etc.)
  - API auto-documentation:
     - see REST doc for idea of everything that can be documented
     - provide API console for experimentation
     - code examples
  - changelog generation
  - add error_uri URL in error messages, pointing to documentation
  - parse comments in IDL file to include them in documentation and changelog.
    E.g. good to describe business-specific IDL functions.

Thorough dev documentation:
  - use jsdoc, esdoc or similar
     - see ESLint rules
  - API engine documentation website

Fake server:
  - fake data generation (using IDL to guess type/constraints), including mixed with real data
  - easy mock server generation for client, by using IDL file

Meta-information:
  - schema retrieval:
     - through API, e.g. /MODEL/schema
     - validation:
        - model's validation JSON schema:
           - Content-Type: application/schema+json [S]
           - can be directly usable with a library like AJV
        - model's IDL schema:
           - should also be linked to by each response as Link: <URI>; rel="describedby" [S]
  - semantic web
  - HATEOAS:
     - see REST documentation for ideas
  - general API "home document":
     - could use OPTIONS with HTTP as well

Output format:
  - offer other formats as data input|output: YAML, CSV, protobuf, XML, etc.
  - content negotiation:
     - allow specifying with protocol-agnostic settings, but also accept standard HTTP content negotiation
       (see Express.js for example)
     - types: format, encoding, language, charset
  - HTTP Content-Disposition
  - option to prettify output:
     - agnostic to output format.
     - should be as featureful as my JSON viewer Chrome extension: highligting, lines folding|collapsing, auto-URL-linker,
       toggle button to show raw, data available in console
     - automatically on when requesting from a browser.

Node.js version:
  - allow using other Node.js version than the latest

Other programming languages:
  - specified by using top-level property "language" in IDL file
  - allow other programming languages in IDL functions
  - allow other programming languages in functions imported by $ref
  - each new programming language must reimplement common functions like underscore.string
  - could also use transpilers, e.g. for TypeScript, etc.
  - make sure JavaScript-specific logic does not apply to other languages

IDL format:
  - IDL file and runOpts file format:
     - allow other formats according to file extension: JavaScript, TOML, HJSON, etc.
     - should also add related custom parsers to $ref utility (json-schema-ref-parser) to handle those types
  - convert Swagger|RAML|API blueprint conf files into IDL format
     - allows those conf files as input, converting them first
  - yamllint the IDL meta-schema
  - create proper IDL file linter, inspired by ESLint
     - standard output (both as a string and as OBJ_ARR)
     - error locations
     - errors documentation URLS
     - autocorrections

Server-client state:
  - e.g. cookies, session, etc.
  - try to avoid having this feature unless necessary

Offline-first:
  - how:
     - do everything the server would do, but client-side
        - i.e. client must know IDL
     - no need for server interaction, except for sync
        - this where server validates that client-side logic happened correctly
        - also where new data is available for other clients
           - which include realtime and conflicts
  - check couchDB and related for ideas about this
  - what about static assets???
     - some offline-first tools choose to bypass backend and let client directly interact with cloud provider

i18n

Privacy feature

Central BaaS API:
  - deploying backends using IDL
  - IDL file's user management (who can modify IDL)
  - must submit not only IDL but also directory/project around it:
     - because compiled IDL still references files, including node modules
     - problem is size of hosting those directories
     - maybe should require GitHub repos, so no need to host

Sysadmin client app:
  - IDL edition:
     - should perform client-side:
        - basic format validation, e.g. YAML linting following by YAML parsing
        - IDL validation
        - IDL test compilation
        - all this should reuse isomorphic server code
  - GUI to central BaaS API

Promotion:
  - commercial website
  - ads:
     - X-Powered-By [S]

IDL migrations:
  - guessing an IDL from existing database.

Positioning:
  - main keywords: BaaS, featureful, easy, generic, stable, open-source
  - market: BaaS
  - main value: backend that is both featureful and easy to maintain
  - target audience:
     - developers, not newbyes
     - no assumptions on particular technologies or business cases
  - main requirements, in order:
     - featureful:
        - any feature a backend can provide
        - high quality design/implementation of each feature
     - easy:
        - maintainability: maintaining, setting up, upgrading, integrating, extending
        - manageability: operating, deploying, scaling, monitoring
        - learnability: documentation, support
        - UI dashboard: good UX, design, usability
     - generic/agnostic:
        - prefer generic over specific, even it lowers efficiency or performance:
           - i.e. interoperability with specific tools (client libraries, databases, etc.) is not paramount
             although nice to have
        - flexibility:
           - allow customizing business logic, with least assumptions about it
           - do not allow end-users customizing API design:
              - prefer forcing good API design over flexibility
              - but encourage contributors to customize API design through generic plugin architecture
     - stable:
        - tested, secure, reliable, available, recoverable
  - configuration:
     - featureful, i.e. many configuration options, which is ok
     - but easiness achieved thanks to:
        - minimal API surface for each option, by sacrificing specificity/efficiency over genericity
        - each option should have good default so they rarely need to be used
   - open source:
      - i.e. no vendor lock-in

4 months to finish features
1 month for quality
1 month for infrastructure

 30  MUST HAVE FEATURES
 3   Database
 1   Complex patches
 2   Orphans
 1   Atomicity/rollbacks
 1   REST
 0.5 JSON-RPC
 0.5 Other operations
 2   Pagination
 3   Authorization
 4   Aggregating
?7   Compatibility layer
?7   Versioning/changes
 4   Static assets
 1   Exception handling
 1   Error reporting
 0.5 JSON schema $data
 1   inputValidation

 10  DATA MODEL
?2   Data correctness
?5   Schema strictness/polymorphism
?3   API base types

 15  NICE TO HAVE FEATURES
?3   Async actions/tasks
?0.5 Alternate ids
 1   Server routing
?4   Realtime
?3   Concurrency conflicts
 1   Rate limiting
 3   Security
 0.5 Request timeout fix

 10  NOT ESSENTIAL FEATURES
 1   Protocols
 0.5 HTTP details
 2   GraphQL
 0.5 create dry runs
?1   Direct database access
?0.5 Database constraints
 1   Model naming
?3   Database transformation layer
?2   Custom commands
 1   IDL functions
 2   Compile-time validation
?3   Custom code
?1   Callbacks/events

 55  SOFTWARE QUALITY
 1   code quality
?5   Caching
?5   Optimization
 0.5 Limits
 1   Streaming
 30  Testing
 2   CI/CD
 6   Logging/monitoring
 2   Dependencies
 2   Refactoring
 0.1 Nodemon exit

 70  DEVOPS/LIVE
 2   Live database
?10  DevOps
?2   System routing
?20  Authentication
?10  Client
 2   GraphQL relay
?5   CLI tool
?25  Admin dashboard

 40  DEV FEATURES
 3   Debugging
 5   User documentation
 10  API documentation
 20  Thorough dev documentation
?2   Fake server
?3   Meta-information

 20  NOT IMPORTANT FEATURES
 4   Output format
 3   Node.js version
 10  Other programming languages
?    IDL format
?    Server-client state

?    Offline-first
?    i18n
?    Privacy feature
?    Central BaaS API
?    Sysadmin client app
?    Promotion
?    IDL migrations
?    Positioning
