Authorization:
  - validateMissingIds middleware:
     - not checked on top-level findMany|updateMany|deleteMany
        - pass args.noMissingIds true to flag that
        - reason (to document):
           - makes it hard to know if a model is absent because ID does not exist, because it was filtered by authorization or because it was filtered by user-specified args.filter
           - it is hard to check for missing models on paginated responses, e.g. for args.filter [{ id: 1 }, {}] or args.filter { id: { in: [10000 ids] } }
     - still checked on:
        - top-level replaceMany
        - top-level *One
        - nested actions, except with a nested filter (feature we don't have)
     - how:
        - if some "modelId" is not present in response:
           - if there was no extra filter applied, i.e. if each args.filter OBJ was only { id: { eq|in } }, throw 404
           - otherwise, query database with args.filter { id: { in "filterIds" } }:
              - if response is not same length as "filterIds", throw 403
  - look again in dryrun create problem
  - model.authorize OBJ[_ARR]|BOOL:
     - OBJ[_ARR] is same format as args.filter:
        - leaves values can be schema functions, but are not given $val/$model
        - OBJ keys are either system variables, user variables or "$model.ATTR"
           - cannot be "$model.id"
     - schema validate this is good format and possible keys.
     - how it is checked:
        - in action layer, after validateStableIds middleware:
           - resolve non-$model keys to BOOL
           - if whole result is false, 403
           - if some $model keys, keep them as "authorizeFilter"
        - in currentData midleware, merge authorizeFilter with args.filter, using top-level "and" node
           - must check it does not messes up the "extractSimpleIds" logic
        - after patchData middleware, check authorizeFilter against args.newData, i.e. for create|replace|patch
     - no more need for args.internal flag that bypasses authorization and pagination, except for orphans
     - requires database features "search"
  - model.authorize.COMMAND,... VAL:
     - behaves like model.authorize VAL except action-wise
     - if any is specified, others default to false
     - patch action uses both "find" and "replace" permissions, with an "and" node
        - no "and" node needed if "find,replace" VAL was specified together
     - model.authorize VAL is normalized (during schema normalization) to { "find,replace,create,delete" VAL }
     - remove schema|model.actions
  - Fobidden actions:
     - calculate forbiddenActions at compile-time, using any model.authorize.COMMAND === false, for each model
        - add "patch" if either "find" or "replace" is included
     - at beginning of first authorization middleware, if forbiddenActions.includes(action):
        - throw 405
        - set header X-Api-Engine-Allowed-Actions: find, ... [S] (depending on forbiddenActions)
     - before sendReponse, add setProtocolHeaders middleware:
        - calls protocolHandler.getHeaders() and filters only the ones starting with X-Api-Engine, removing the prefix and making it camelCase
        - calls protocolHandler.setProtocolHandlers({ headers }) with those headers, which set protocol-specific headers using protocol-agnostic ones
           - for HTTP, use X-Api-Engine-Allowed-Actions: find, ... [S] to set Allow: GET, ... [S]
     - do not show forbidden actions in GraphQL schema
  - models.all:
     - similar to models.default except:
        - merged instead of being overidden
        - how the prop is merged is specific to each prop
        - only limited set of valid props:
           - models.all.authorize VAL

Orphans:
  - validation:
     - attribute targeting another model have limited validation:
        - the reason: they might be deleted|modified when the model they target is deleted, and that model deletion should not have to care about it, but in the same time we don't want invalid models
        - forbidden: required, minItems, dependencies, contains, custom validation, being part of $data cross-attributes validation
  - authorization:
     - should bypass authorization and pagination (i.e. be "internal"),
       all of them: find dead links, remove dead links, bidirectional links
     - results should not be reusable though (i.e. not in results), to avoid normal queries bypassing authorization
        - however remove dead links and bidirectional links do need to use results???
        - should try to reuse results (for perf optimization) if query should have been authorized even if not bypassed???
        - divide between results and forbiddenResults???
        - transform patch action into replace action right away to avoid patch middleware???
        - only for read actions (write actions do not reuse results), both currentData, normal read and extra
          selection in write actions
     - actions should have no args.select, so they are not in output
     - are those actions topLevel???
  - find dead links:
     - for create|patch|replace commands
     - find all ids+modelName that:
        - have attr.target !== undefined
        - are defined in args.data
        - are a "leaf", e.g. in args.data { childA, childB: { ... } }, childA, not childB (since it will be created, it cannot be a dead link)
     - for:
        - create|patch:
           - run in parallel of currentData middleware
           - merged to same output, i.e. to results
           - should reuse each other (results and pendingPromises)
           - start at same time. end when both are over.
        - replace:
           - only args.data that are being modified, i.e. defined in args.data and different from results
           - i.e. need to be run after currentData
           - but output still merged to results
     - goal: firing 404 if dead link
  - remove dead links:
     - middleware fired after currentData middleware, before mergePatchData middleware
     - on delete commands
     - first search for models with dead links:
        - do find commands with args.filter { attr: { containsOrIs: ids } } where ids are the models being deleted, on the models that might target them
           - if the model being deleted contains bidirectional links, can optimize with faster command (i.e. filter directly by id) because the models ids are known
        - add args.filter { id: { neq: ids } } to exclude models already being deleted, i.e. already present in delete actions
          results
     - merge the find commands results to results
     - then create patch actions to remove dead links
        - do not fire those patch actions, only stack them up at the end, and resolveWriteAction will take care of them
  - bidirectional links:
     - attr.reverse "attr2"
        - schema validate (before normalization)
           - forbid attr.reverse if attr.target is not defined
        - schema pre-normalization:
           - attr2.reverse "attr" is added, unless already present. I.e. link is truly the same on both ends.
        - schema validation (after normalization):
           - model2.attr2 must exist, and have attr2.target defined
           - attr.target and attr2.target must be defined and point at each others models
           - attr.reverse "attr2" and attr2.reverse "attr" must be defined and point at each other (using also attr[2].target)
     - middleware:
        - on create|patch|replace commands
        - after currentData middleware, before removeDeadLinks middleware
        - fired for any attribute:
           - with attr.reverse defined
           - and that is being modified, i.e. defined in args.data and different from results
        - first merges into results some find commands results:
           - all ids being modified should trigger find commands
              - except if already present in results.
                This should cover all ids being added, since findDeadLinks middleware already searched for them.
        - then create patch actions to modify all the reverse ids
           - both the ones being added, and the ones being removed
           - do not fire those patch actions, only stack them up at the end, and resolveWriteAction will take care of them

Pagination:
  - problems (should document them in code):
     - parent actions are performed several times while going through the child action pages:
        - this is inefficient
        - this creates duplicates results, i.e. clients need to remove them
        - does not work when parent action cannot be performed twice, e.g. delete or create
     - this creates one pagination output metadata per action, which is more complex
  - solution: max models limit:
     - top-level actions are paginated, i.e. limited by runOpts.pageSize
     - nested actions are not paginated. Instead, two limits:
        - maxModels:
           - each operation can contain maximum NUM models, including top-level and nested actions. This is operation-wise, not action-wise.
              - does not include duplicate models.
                E.g. if fetching two nested models with same id, consistency middleware will remove duplicates, so they will not impact database load, and should not be counted in limit.
           - NUM is runOpts.maxModels.
              - defaults to 100 * runOpts.pageSize (not args.pageSize)
           - for many-to-many nested operations that hit that limit, two solutions:
              - decreasing args.pageSize
              - split the nested operation into several flatter operations
        - each nested action OBJ_ARR output length must be maximum pageSize:
           - this is to avoid using nesting to circumvent pagination
           - should count duplicates, not ignore them
           - it is for each OBJ_ARR, not whole nested action. E.g. if a parent has 100 OBJ_ARR children, each child should be under that limit,
             even though they will all trigger one single action, which might return more than that limit
     - for both limits:
        - if over limit, throws error
           - should trigger rollback
           - if it can be thrown before write actions are fired, it is even better as it avoids rollback
        - should use database offset|limiting NUM+1 in child actions to limit database load
        - for all actions, including createMany and replaceMany
  - general (top-level actions):
     - *One actions: no pagination
     - createMany, replaceMany:
        - no pagination args nor output metadata
        - but if args.data length (top-level only) > runOpts.pageSize, throws error
        - do not allow args.page_size
     - findMany:
        - either:
           - offset vs cursor:
              - is offset pagination if args.page !== undefined
              - default is cursor pagination otherwise, with args.after ""
              - document difference with offset pagination: not random-access, but stronger consistency/isolation
           - offset pagination:
              - args:
                 - page, page_size: same as current system
              - metadata.pagination:
                 - page, page_size: same as current system
                 - has_previous_page, has_next_page: same as current system
                 - total_size NUM, page_count NUM:
                    - calculated by doing an extra count database query
                    - always performed, but on offset pagination only
           - cursor pagination:
              - args:
                 - page_size: same as current system
                 - after, before: same as current system:
                    - including:
                       - how cursors are calculated
                       - "" cursors for beginning and end
              - metadata.pagination:
                 - page_size: same as current system
                 - has_previous_page, has_next_page: same as current system
                 - previous|next_token "CURSOR":
                    - instead of current system where each model has its own token
                    - those two cursors are the cursor calculated of the last and first model
                    - undefined if no previous or no next page
                    - cursors should store oArgs and use them for comparisons, not normalized args.
                       - clients should submit the same args.filter|order_by (instead of not submitting any, as it is now), i.e. only add args.after|before.
                 - first_token|last_token: always ""
     - patchMany, deleteMany:
        - like findMany except for the following points
        - no offset pagination
        - only forward cursors, i.e. no args.before nor  metadata.pagination.previous_token|first_token|last_token. metadata.pagination.has_previous_page is always false.
        - paginates the write database query itself:
           - i.e. cannot patch nor delete too many models at once
           - do it by paginating first internal find action, then use the ids in replace|patch|delete actions
              - i.e. replace|patch|delete actions do not use pagination middleware for cursor pagination directly
  - middleware:
     - handles all types of pagination: max args.data length, cursor pagination, offset pagination.
     - e.g. max args.data length not done during args syntax check middleware
  - runOpts.pageSize:
     - instead of the three runOpts currently used, i.e. maxPageSize, defaultPageSize and maxDataLength
     - defaulting to 100
     - args.page_size can be used to change pageSize:
        - but can only be lower
        - integer, minimum 1
        - with findMany, patchMany, deleteMany
     - 0 to disable pagination:
        - i.e. no paginating, pagination args nor output metadata
        - args.page_size can be anything
  - metadata:
     - should not be selectable, i.e. always fully rendered in output
     - operation-wise, i.e. not action-wise, nor model-wise
     - in output, is sibling to "data", in both GraphQL and REST, i.e. { data OBJ, metadata OBJ }
     - should be undefined if args.silent true
     - go through the whole codebase looking for "metadata" because there's been quite some changes
  - relay.js:
     - requires model-wise cursors, which is too slow, and too complicated for users, i.e.:
       - only expose Relay.js cursors as opt-in
       - only first and last cursor are calculated. The others simply append an offset to the first cursor.
          - it is not as isolation-safe, but is much faster and only meant for Relay.js
  - protocol-level:
     - use Range [C] and Accept-Ranges [S] for pagination
        - e.g. transforming them into pagination regular arguments
     - first|last|previous|next:
        - like first|last|previous|next_token, but prepended with URL to perform the query
        - for GraphQL, need to put query in the URL query string
        - also as HTTP Link: <URL>; rel="first|last|prev|next|self[ ...]"
  - system defaults:
     - move all of them to their respective middleware

Complex patches:
  - provide MongoDB-like patch syntax for patch action, including array manipulation

Atomicity/rollbacks:
  - in resolveWriteActions(), replace Promise.all() by something that waits for all promises to complete,
    even on failure, and return all of them
  - add rollbackInfoBefore and rollbackInfoAfter middleware just before and after databaseExecute,
    which uses a variable "rollbackInfo" shared by all write commands
  - if any command throws, set rollbackInfo.hasFailed true
  - in rollbackInfoBefore middleware, if rollbackInfo.hasFailed true, throw error
  - in rollbackInfoAfter middleware, do rollbackInfo.completedCommands.push(modelName)
  - resolveWriteActions() waits for completion of all commands, failure or not. Then, if any failed, it:
       - does a rollback on each rollbackInfo.completedCommands
       - wait for rollback completion
       - rethrow initial error
       - if rollback itself threw an error, set rollback error stack trace to initial error.failedRollback STR,
         using error.extra
       - what if an error happened in the middle of a database command, i.e. command is only half completed???
          - this means rollback commands could not assume commands were completed
          - i.e. is it even worth it waiting for all commands completion???
          - should it do extra find commands to figure it out???
  - rollback reverse the command, i.e.:
       - create -> delete
       - patch|replace -> replace with currentData
       - delete -> create with currentData
    
REST:
  - including:
     - selecting
        - including nested selections, to mimic GraphQL
        - reuse the "all" special attribute
     - populating
        - use args.select "attr.all" instead???
     - right status codes, e.g. 201 + Location [S]
     - Accept-Patch [S]
     - return value aliasing, like in GraphQL, by using args.select "ATTR=ALIAS" (already implemented)
     - same output as GraphQL, except errors OBJ_ARR is error OBJ
  - use user-friendly format
  - argument values (including for args.filter):
     - can be omitted, to mean true
     - quoting is optional, i.e. 'STR' is same as "STR" and STR
     - should transtype to number|boolean if argument allows numbers and value looks like number|boolean
     - if STR might contain forbidden chars, should URI encode:
        - try to minimize number of arguments that might contain forbidden chars
     - when value is ARR|OBJ, can either use:
        - JSON stringified then URI encoded: best for programmatic usage
        - ?ARG[.NUM|VAR]...=VAL: best for debugging
  - arguments can also be in headers:
     - X-Api-Engine-ARG=VAL
     - treated the same as in URL except:
        - no need for URI encoding
        - no ARG[.NUM|VAL]... notation
        - has less priority than query string
     - is more semantically correct as it keeps URL clean, but harder for debugging
  - fix user documentation to make it more operation-agnostic

JSON-RPC:
  - add error code number to error reasons
  - no batch requests
  - both versions 1.0 and 2.0. Same endpoint, but use version field to distinguish.
  
Aggregating:
  - are just other commands, following similar logic as the others
     - should also think of which other commands it requires, and which one it implies
  - e.g. countUsers, groupUsers, etc.
  - arg.distinct "ATTR": no duplicates
  - is basically asking for server-side multi/single-ATTR/OBJ operations, so maybe try to allow client to specify
    it as JavaScript, parsed as MongoDB aggregate object???
  - think of interaction with pagination

Compatibility layer:
  - reporting deprecation
  - breaking changes:
     - notify when schema change introduces breaking change (e.g. graphql.js provides that)
  - think about how to version schema file
  - autoversioning:
     - might be related to breaking changes feature
  - migrations helpers
     - when changing schema constraints, should migrate data so they conform to new constraints
     - when adding default|transform, should migrate data, otherwise when doing patch with empty data,
       or find followed by replace, both should be idempotent, but default|transform would actually be applied
  - should it generate redirects when name has changed?
  - feature flags

Versioning/changes:
  - can probably link together versioning, changes and undelete features
  - versioning:
     - on any model modification
     - should allow restoring
        - including undeleting:
           - use query parameter "show_deleted" and model attribute "deleted"
     - should allow searching
     - maybe, instead of introducing new goals/commands, use special attributes to search or modify
  - listening for changes:
     - should emit change events, but not know how they are used, i.e. not know the subscriber side
        - alternative is to emit events on ApiServer eventemitter for local consumption.
          Less decoupling, but easier to implement and consume.
          Might even be able to do both, e.g. add a module that translate local events into remote events.
     - should allow listeners to catch up if they missed some events because of network problems
     - should allow listeners to target specific: model, attribute, value, condition on value (e.g. value < 5),
       request context, request user, etc.
       Does not mean emitter perform those checks, but that it allows them to be performed
     - make it easy to integrate with SaaS integrations, e.g. "use this service to send an email"
        - standardize/simplify the interface to make it easy to create integrations
  - separate from rest:
     - done after the request was handed back to client, i.e. no impact on performance
     - done in different database
        - could be single table with streams of changes
  - can problably use standard diff format, e.g. JSON patch or JSON merge patch.
    Could also store models in full
  - can limit max number of versioned models by time or by absolute number or a combination
     - need to make sure change listeners can still get all events without model being removed from their reach
  - maybe use HTTP memento
  - maybe use semantic links, e.g. Link: <URI>; rel="alternate|canonical|latest-version|working-copy|predecessor-version|
    successor-version|version-history" [S]
  - must version schema file format itself, and also apiEngine itself

Static assets:
  - take inspiration from existing ones, probably reusing one
  - take inspiration from Express sendFile()
  - think of Content-Disposition
  - integrate GraphiQL with this
  - server-side templates serving
     - including isomorphic server-side renderer

Exception handling:
  - error reasons:
     - rationalize|reduce error reasons:
        - should be not about guessing where error was internally triggered, but in terms of how client should respond.
        - e.g. it does not matter if InputValidation error is because args.data is wrong or args.filter, what matters is that
          client displays that input is wrong.
  - should add `extra` information to any throw ERROR that possess interesting information.
     - all errors with the same `reason` should expose the same `extra` variables
  - document available error responses types, error `extra` and status codes
  - add all `generic.title`
  - simplify JSON validation utility

Error reporting:
  - improve error messages for JSON schema composed types: schema.contains|propertyNames|not|anyOf|oneOf|if|then|else (others are fine)
  - error response should include parsable info for validation errors:
     - data that failed
     - data path: either as JSON path (def) or as JSON pointer (using a runOpt)
     - rule (e.g. 'exclusiveMinimum')
     - operation context (e.g. action, action_path)
     - any other relevant info
  - add `locations` attribute to GraphQL errors, and possibly other documented in my GraphQL doc
  - correction suggestions:
     - e.g. wrong attribute name or wrong args.filter operator
     - provide list both in error message, and as parsable `extra` info
     - if possible, use levenshtein distance to guess possible typo suggestion
  - variants: if data validation fails because of an attribute that was generated as a variant, error message must include information that it was generated as such, otherwise it's confusing for end-user
  - request timeout error includes very little information (e.g. does not include protocol) because it is fired in
    an early middleware

JSON schema $data:
  - $data notation is a bit cryptic and prone to error with JSON pointers.
     - should replace to something more user-friendly, where user just need to specify the sibling model's attribute name
     - at the moment, $data is removed before schema validation, this would need to be changed
     - validateMap would need to convert this notation to $data, for ajv to work
     - error reporting does not currently work with $data, because ajv does not translate $data into the actual referred
       data. E.g. it says "should equal [Object object]" because { $data: STR } is kept as is

inputValidation middleware:
  - like dataValidation middleware but before any server-side data transformation:
     - i.e. beginning of command layer
     - reuse attr.validation like dataValidation middleware
        - reason we do not have two different sets of validation: data stored in database should be valid input, and also having two sets adds two much complexity and room for errors to end-users
     - gives 4** client-side errors, while dataValidation middleware gives 5** server-side errors
     - input only (like dataValidation middleware)
     - should try to memoize|reuse per request (to avoid memory leaks) between inputValidation and dataValidation middlewars

Schema strictness/polymorphism:
  - think about additional properties (not specified in schema), and whether to allow them in input, and in output:
     - at the moment, they are allowed
     - if want looser schema, think of impact on:
        - validation: see related JSON schema keywords: prohibited, additionalProperties, patternProperties, patternRequired,
          propertyNames, additionalItems
        - GraphQL schema
  - should it be allowed to specify an unknown ATTR in order_by, reduce, etc.?
  - type object|object[]:
     - probably not a good idea because:
        - not taken into account in limits (max attribute size, max number of models, max attributes per model)
        - schemaless, e.g. no description, etc.
           - if users need dynamic properties, can add this feature to models without having to add "type object"
        - object JSON schema validation is not super simple
        - denormalize where everything less in the engine is otherwise normalized
           - e.g. could not query those models, because not top-level
     - upside: less database queries
  - allow mixed arrays:
     - i.e. using schema.items SCHEMA_ARR instead of schema.items SCHEMA
     - also schema.additionalItems
     - re-verify error messages for those two properties
  - overloading, i.e. union types:
     - in GraphQL, use "union" maybe.
     - could be done by passing array of schema.type or schema.model
        - there is already some basic support for schema.type array in schema validation
  - subtyping:
     - both nominal (e.g. guessing type using an attribute as differentiator)
     - or structural (e.g. guessing type from which attributes are there, or which are their types)
     - in GraphQL, use "interface" maybe
     - add the moment GraphQL fragment "on TYPE" are noop, i.e. always return true whatever TYPE is

API base types:
  - attributes that are objects (or array of objects) but not models:
     - at the moment, are not allowed, because we want the user to take advantage of using models: creates endpoint,
       documentation, etc.
     - think whether this is a good idea?
     - this means JSON schema object validation will never be used (although technically allowed)
  - enum type:
     - with possibility to add metadata (description, deprecation, etc.) on each value
     - think if needs to allow transform between database representation and API representation
     - might use schema.constant|enum in JSON schema
     - might use GraphQL enum
  - new types beyond JSON ones:
     - must still be serializable|parsable from JSON
     - must be of the correct type when in API layer
     - must work with arg.filter normalization
     - must work with pagination cursor serialization
     - might want special GraphQL types for proper schema, e.g. special GraphQL scalar types
     - validation layer must handle them correctly
     - e.g. undefined, function, Infinity|NaN
     - allow custom types

Upgrade MongoDB doc:
  - upgrade doc of MongoDB, when it goes to 3.6 (see my doc for more info)

Async actions/tasks:
  - task management (restarting, retrieval, etc.)
  - Async actions must be well thought as they slow down requests:
     - there should be jobs reported to users and users should be able to control max wait time.
     - when this is figured out, think of how async schema functions would work within that
        - consider that paramsRef is directly mutated, i.e. might not be thread-safe
     - try to think if need generic async actions output (e.g. HTTP 202, Prefer: respond-async [C], Prefer: wait=NUM [C])
  - see REST doc for more info

Alternate ids:
  - e.g. a query can either use machine-friendly `id` attribute, or human-friendly `name` attribute

Server routing:
  - compare with existing libraries, and see if should reuse one and/or their features

Realtime:
  - protocol-agnostic, i.e. WebSocket protocol is just one option
  - subscriptions (on-demand or automatic after a query|mutation)
  - maybe something like:
     - create an endpoint to setup connection, where specify if want automatic or on-demand
     - if on-demand, must then pass extra parameter to operations (via arguments)
       to specify want to subscribe

Concurrency conflicts:
  - locking or MVCC (automatic merge conflicts)
  - preconditions, including HTTP (e.g. If-Match [C])
  - errors, including HTTP 409
  - see concurrency chapter in to_learn.txt
  - problem with concept of "changed" in variants middleware:
      - e.g. if clientA fetches model as { a: 1 } then clientB saves it as { a: 2 }, then clientA saves it as { a: 2 } as well,
        "a" will not be considered "changed", although it should, because "changed" is not about which attributes changed from
        a server perspective, but about which attributes a client tried to change, i.e. from a client perspective.
      - e.g. variants middleware won't work properly, e.g. clientA fetches model as { oldA: 1, newA: 1 }, then clientB
        saves it as { oldA: 2, newA: 2 }, then clientA saves it as { oldA: 1, newA: 2 }.
        clientA intents here to chamge variant using "newA", but "oldA" will be used instead

Rate limiting:
  - should be shared between server instances
  - maybe at API gateway-level
  - see HTTP doc for standard headers and status codes

Security:
  - TLS
  - CORS
  - XSS
  - CSRF
  - general utilities, like "helmet"
  - should be protocol-agnostic as much as possible

Request timeout fix:
  - it currently uses Promise.race(), which does not cancel other promise when one fails,
    i.e. request keeps being processed even after timeout error response has been sent
      - instead, requestTimeout should only set a request-wide variable on setTimeout() callback.
        Before each middleware, this variable should be checked, and if set, throw an exception.
  - should call SERVER.setTimeout(NUM) too
  - requests that error should not have already modified the database, or should rollback
     - i.e. request timeout cannot happen after no rollback is possible anymore
     - do this by passing setTimeout() return value to mInput, and calling clearTimeout() after last command layer completed
  - think carefully of how much timeout should be:
     - including when parsing request bodies as big as the limit

Multiple databases:
  - allow using the same database adapter several times but with different options
     - how this is specified in the options is problematic:
        - using OBJ_ARR in options is not currently supported, and is creating many problems, so should avoid
        - using e.g. db.ADAPTER[_*].* notation for extra adapters is hard:
           - each adapter options should be parsed and validated the same in /src/options/
           - extra dynamic options should not appear in CLI --help message
           - CLI --help message should document that dynamic options can be added
           - CLI should be aware of dynamic options, so it parses them (e.g. adapter.models to an array)
           - maybe use YARGS.hidden()

File formats:
  - make request payload use genericLoad():
     - i.e. separate MIME-specific parsing from protocol handlers, which would then only return the request payload
       as a string
     - do not use body-parser anymore, but reuse its code
     - each generic format should now declare:
        - a list of MIME types, including a prefered one (for response payload)
        - whether it should be used for conf files parsing, request|response payloads, or both
  - make response payload use genericLoad() on collection|model|error|object serializing
  - add generic formats:
     - TOML
     - INI
     - CSV
     - TSV
     - DSV
     - HJSON
     - JSON5
     - JavaScript
     - XML
     - protobuf
     - others

HTTPS support

HTTP/2 support

Proxies:
  - make sure it works well with proxies

HTTP details:
  - Add support for 204 status code, with any write goal
  - HTTP server events "checkContinue" (for 100-continue [C]), "connect" (for CONNECT), "upgrade" (for Upgrade [C])

GraphQL:
  - problem with polymorphism in GraphQL schema:
     - nested attributes can either be a string or a nested object, in both selection and args.data
     - GraphQL does not allow union of scalar and object
     - also, GraphQL does not allow union types in input
     - this is problematic as GraphiQL:
        - shows queries as invalid
        - autocorrects queries, e.g. turning nested_model selection into nested_model { id }
     - same problem with args.filter, since args.filter.ATTR can be either VAL or a complex object
        - that complex object should depend on the type of VAL:
           - the leaves should be of the same type, e.g. { eq STR } if VAL is string
           - some operations are available only for specific types
  - fragments:
     - if we do not allow any kind of polymorphism, consider not allowing fragments, as they only make sense with polymorphism
     - fragment "on type" currently does not do anything
     - all fragments must be used
     - no recursive fragments
  - variables:
     - variable declarations does not currently do anything
     - no duplicate variables
     - no unused variables
  - introspection:
     - support __typename
     - support __type(name: 'TYPE')
     - support mixing GraphQL introspection query (e.g. __schema) with non-introspection query
     - maybe do this by reusing code from Graphql.js
  - go through GraphQL spec validation chapter again

Create dryruns:
  - problem:
     - create* dryruns must fail if models already exist
     - database adapters should be agnostic to this, i.e. should just perform find queries to check for model existence
     - we want to avoid doing one query for each model, but still handle the situation where some models exist,
       while some others do not
  - solution:
     - create* dryruns transform to a find query (like delete dryruns)
        - add args.filter.id using args.newData. Remove args.filter as soon as database check finishes, so that next middleware
          do not use it
        - add args.createDryRun true so that databaseExecute middleware knows
     - a findMany query is performed:
        - if *all* model ids returned a 404, catch the error and return same response as replace dryrun
           - this requires database adapters to return *all* failing ids when throwing a 404, which I am unsure whether
             it is too much to ask???
        - if *any* model id does not return a 404, throw same error as when trying to create a model that already exist

Model naming:
  - schema.models.MODEL.names STR_ARR:
     - instead of MODEL.model
     - while MODEL is how model is refered to in schema, and is stored in database, this is about how model is communicated to
       client (e.g. GraphQL action names or REST URLs)
        - attr.type must refer to MODEL, not MODEL.model, to avoid name conflicts with JSON type,
          i.e. so that models can be called "string", "array", etc.
     - def to ["MODEL"]
     - must be schema validated
     - if ARR.length > 1, extra ones are aliases
        - when an alias was used, should communicate canonical name (i.e. ARR[0]) to client using header: X-Api-Engine-Model-Name: MODEL [S] (protocol-agnostic)
          and Content-Location: MODEL [S] (HTTP-specific)

Database transformation layer:
  - before database action middleware
  - e.g. one model in two tables, two models in one table, database-specific info, for both input|output
  - should allow single server to use multiple databases with different technologies (e.g. MongoDB + Redis) too
  - possibility: using variants.ATTR.ATTR2 (instead of variants.ATTR) with ATTR being a nested model
     - in that case, need to think about consequences on $model system variable

Custom actions:
  - must be declared in schema file
  - are global, not model-wise? if model-wise, nesting?
  - think of nested actions
  - middleware at beginning of action layer
  - must contain:
     - handler function:
        - can call core actions with normal args
        - get input from operation layer
     - information to build its GraphQL schema, and do input|output validation

Schema functions:
  - think if should use them in other parts of schema
  - Add system variables related to device|browser detection
     - add them to logging requestInfo too
  - make system variables immutable but:
     - not the user variables, cause they are external and might mutate
     - only once per system variable per request, because setting immutability is slow
        - what about $model???
  - using JSON references with plugins:
     - plugins can only inject inline functions, not JSON references:
        - because they cannot be sure of the relative path from the schema file to the injected JSON references
     - plugins can only use as options inline functions, not JSON references:
        - because JSON references are not compiled to actual JavaScript functions during plugin injection
           - e.g. author plugin opts.currentUser has to be inline, cannot be a JSON reference
        - maybe a solution would be to normalize any JSON reference passed as option to a plugin, to a user variable,
          and pass that user variable string to the plugin instead

Compile-time validation:
  - schema validation should not use JSON schema, but fastValidation to give better error messages
  - should schema validate that validate.required|dependencies are only used at top-level:
     - because of JSON schema recursion, it's actually currently possible to do validate.allOf.required
     - this will crash, because we manipulate those properties at top-level, before calling ajv
  - according to each attr.type:
     - should validate that attr.validate only contain keywords for that type
     - attr.default is of that type
     - careful because attr.type is only decided after normalization
  - schema functions:
     - validates when can that take either schema function or a constant of the same type as the attribute,
       e.g. in attribute.default|transform
     - make sure schema functions does not use unknown params, which is challenging because:
        - function body might not be readable, e.g. if function is bound
     - perform static analysis, e.g. linting
     - maybe validate complexity, e.g. max length

Custom code:
  - separating code into several packages
  - using a more plugin-oriented architecture:
     - allow users to write support for new protocols, operations, etc.
     - allow users to add|remove middleware
     - should do this with a decorated FUNC(APIENGINE)->APIENGINE, as opposed to using a run option
        - decorated APIENGINE should keep all features, including CLI
     - allow integrations (see above)
     - standardized how to do all this, e.g. with specific tooling, and with specific npm tags to easily list them

Callbacks/events:
  - not sure if this is a good idea
  - on new|finished layers
  - while events are async, callbacks are sync and allow modification of input/output

CLI:
  - dynamic options, such as db.mongodb.opts, does not work with CLI

systemInfo event payload:
  - add info about:
     - databases|protocols|formats|operations and their options
     - databases stats (e.g. dataset size, memory, load, etc.)

RegExp compatibility:
  - figure out common subset of RegExp flags and features supported by all database adapters,
    so that it is the same across database adapters

Code quality:
  - auto-beautifier
  - escomplex

Caching:
  - protocol-level caching, including HTTP caching
     - allow specifying with protocol-agnostic settings, but also accept|produce standard HTTP caching
       (see Express.js for example)
  - automatic request caching, and invalidation:
     - between API and database
     - between client library and API
     - between client and client library, e.g. creating a client library that gets push from server on invalidation,
       so it does not even perform any request
     - saved on key-value store, so can be shared between instances
  - delta encoding

Optimization:
  - size of array returned by *Many actions should not impact too much response time:
     - e.g. at the moment many things (e.g. attr.transform) is run once per model, where it could instead be run once for
       all models
  - look for memory leaks
     - check for memory leaks in memoize(), i.e. new requests should not increase memoize() memory retention
  - do performance profiling to see which parts are slow
  - memoize schema functions run per request. Instead of stringifying ifv, use === comparison
  - concatenate schema functions together:
     - e.g. `test` in transform/value can be { test TEST, value VALUE } ->{ value (TEST ? VALUE : $val) }
     - e.g. array of transform/value can be concatenated into a single schema function
     - apply schema functions in batch, i.e. instead of applying same transform to several models of same collection, transform
       schema function to $val.map(FUNC) and apply on collection instead
  - maybe use perf_hooks module instead of process.hrtime() + OBJ.measures for perf monitoring:
     - not sure if this is a good idea:
        - timerify() does not wait for async functions
        - FUNC.name is messed up by timerify()
        - 'mark' and 'measure' must be cleaned, which can be problematic if error happens in the middle of request middleware
        - mark() + measure() + getEntries() is a bit verbose and slow
        - 'make' and 'measure' names must be specific for each request, to not mix each request perf
     - on the flip side, this avoid explicit shared variable OBJ.measures
     - performance.nodeTiming for startup perf does not work, because it is as a library, so it would encompass the calling code

Limits:
  - when project is more stable, need to reconsider each limit:
     - to limit the hassle it is for end-user. E.g. pagination size too slow means paying for several requests, and longer overall request, which is frustrating.
     - while still having an ok processing time for average big requests
     - consider the limits of the DBaaS, and of the database they use, to avoid hitting those limits

Streaming:
  - input|output streaming:
     - protocol-level, e.g. HTTP
     - format-level, e.g. ndjson or YAML streams
  - Expect: 100-continue [C]

Testing:
  - unit tests:
     - test coverage
     - data-driven tests
     - fuzz testing
  - integrated tests
  - load testing
  - performance testing
  - greenkeeper

CI/CD

Logging/monitoring:
  - logging:
     - dashboard
     - go through docs and to_learn
     - separate into distinct modules functions that take engine event payload format and
       transport to a specific place. I.e. act like logging plugins.
  - monitoring:
     - host metrics
     - alerting
  - health monitoring
     - status page
     - health/ping endpoint
     - think of added costs in a FaaS setup, and repercursion on pricing
        - e.g. pinging every second would cost 100ms task per second
  - distributed request tracing
  - analytics:
     - tell which models are used, which attributes, which versions, which params, etc.
     - might be able to build on logging feature ("call" type) for that

Dependencies:
  - package.json linting
  - learn more about npm, yarn, etc.
  - deprecation/security automatic check
  - changelog for the engine project itself
  - dependencies upgrades: choose strategy and tools (like greenkeeper)
  - deprecation for the features of the engine project itself

Less code:
  - replace some code by libraries:
     - /src/utilities/functional/
     - other /src/utilities/
     - /src/json_validation/

ES modules:
  - use ES6 import/export, when supported natively by Node.js without --experimental-modules, import() and import.meta supported
     - rename *.js to *.mjs
     - use import.meta instead of __dirname|__filename
     - use import, export and import()
     - fix ESLint rules for that
     - no need for 'use strict' anymore nor ESLint impliedStrict true
     - use shrimpit

Nodemon exit in production mode:
  - when server.keepAliveTimeout is left to its default value (i.e. 5000), and Nodemon is running, and a request
    has just been fired (i.e. socket is still alive because of timeout), hitting CTRL-C will fail at freeing the
    socket, i.e. restarting right after will fail.
  - a former solution I had was to fire process.kill(process.pid, 'SIGUSR2') on shutdown event, but this was problematic:
     - if several servers are run at once (with or without Nodemon), this will make the first one that finished exiting
       abrupt the others
     - it adds Nodemon-specific code

Live database:
  - DBaaS
  - backups
  - high-evailability
  - scalability

DevOps:
  - PaaS/FaaS
  - serverless:
     - since AWS lambda does not reuse Node REQ|RES, possible solutions:
        - treat AWS lambda as a different protocol, alongside HTTP
           - problem: there might be code duplication for the HTTP-related code, e.g. query string parsing
        - create utility that converts AWS lambda input to REQ/RES
  - easy to spawn multiple environments (stage, A/B testing, etc.)
  - Docker container
  - canary
  - rolling releases

System routing:
  - maybe as API gateway
  - load balancing
  - autoscaling

Authentication

Client:
  - Make some parts isomorphic, e.g. data validation, schema file loading, schema validation, etc.
  - Integration with frontend frameworks, client auto-generation

GraphQL relay:
  - must add clientMutationId, see https://facebook.github.io/relay/graphql/mutations.htm
  - must follow https://facebook.github.io/relay/graphql/objectidentification.htm

CLI tool:
  - for doing both administration, schema edition, or custom functions

Admin dashboard:
  - like Mr.Wolf, but automated
  - for content management, basically a GUI to the API

Debugging:
  - use GraphQL voyager instead of GraphiQL
  - graphiql should be according to Accept [C] (not route) with potential override
    with query variable like 'raw' to see raw result
  - HTML interactive output format when requesting from a browser

User documentation:
  - interactive examples
  - ability change examples protocol, interface, schema format and programming language

API documentation:
  - description:
     - build it using not only schema.description, but also schema.examples, schema.title and schema.* related to validation.
     - should be done during schema compile-time transformation
  - printSchema():
     - better sorting
     - maybe change endpoint or way to get there.
     - improve syntax highlighting.
     - also maybe offer option to show full version, and offer simplified version by default,
       e.g. showing only one action, and not showing variants (nested, singular|plural, etc.)
  - API auto-documentation:
     - see REST doc for idea of everything that can be documented
     - provide API console for experimentation
     - code examples
  - changelog generation
  - add error_uri URL in error messages, pointing to documentation
  - parse comments in schema file to include them in documentation and changelog.
    E.g. good to describe business-specific schema functions.

Thorough dev documentation:
  - use jsdoc, esdoc or similar
     - see ESLint rules
  - API engine documentation website

Fake server:
  - fake data generation (using schema to guess type/constraints), including mixed with real data
  - easy mock server generation for client, by using schema file

Meta-information:
  - schema retrieval:
     - through API, e.g. /MODEL/schema
     - validation:
        - model's validation JSON schema:
           - Content-Type: application/schema+json [S]
           - can be directly usable with a library like AJV
        - model's schema:
           - should also be linked to by each response as Link: <URI>; rel="describedby" [S]
  - semantic web
  - HATEOAS:
     - see REST documentation for ideas
  - general API "home document":
     - could use OPTIONS with HTTP as well

Other database adapters:
  - add support for other databases
  - each adapter:
     - should throw DB_MODEL_CONFLICT (create only) and DB_ERROR (any action) errors
     - should not throw when targetting an id that do not exist
     - potentially ADAPTER.id.name and ADAPTER.id.default()
        - prefer UUIDs so it looks consistent across databases
     - ADAPTER.type|title|description
     - ADAPTER.features:
        - and whether they should be checked startup-time (/src/run/database/) or query-time (validateFeatures middleware)
     - ADAPTER.connect|disconnect()
     - ADAPTER.query()
     - using undefined|null in both read and write
     - like REGEXP test should not be anchored, unless ^$ is used
     - should try to use the same option names accross adapters, e.g. port, host, username, password
     - should reconnect if connection is lost
        - should try to reconnect several times, but should give up quickly if does not work
        - devOps should be used instead to restart the machine

Other operations:
  - implement other operations, beyond GraphQL, REST and JSON-RPC

Output format:
  - content negotiation:
     - allow specifying with protocol-agnostic settings, but also accept standard HTTP content negotiation
       (see Express.js for example)
     - types: format, encoding, language, charset
  - HTTP Content-Disposition
  - option to prettify output:
     - agnostic to output format.
     - should be as featureful as my JSON viewer Chrome extension: highligting, lines folding|collapsing, auto-URL-linker,
       toggle button to show raw, data available in console
     - automatically on when requesting from a browser.

Node.js version:
  - allow using other Node.js version than the latest

Other programming languages:
  - specified by using top-level property "language" in schema file
  - allow other programming languages in schema functions
  - allow other programming languages in functions imported by $ref
  - each new programming language must reimplement common functions like underscore.string
  - could also use transpilers, e.g. for TypeScript, etc.
  - make sure JavaScript-specific logic does not apply to other languages

Schema format:
  - convert Swagger|RAML|API blueprint conf files into schema format
     - allows those conf files as input, converting them first
  - yamllint the schema meta-schema
  - create proper schema file linter, inspired by ESLint
     - standard output (both as a string and as OBJ_ARR)
     - error locations
     - errors documentation URLS
     - autocorrections

Server-client state:
  - e.g. cookies, session, etc.
  - try to avoid having this feature unless necessary

Offline-first:
  - how:
     - do everything the server would do, but client-side
        - i.e. client must know schema
     - no need for server interaction, except for sync
        - this where server validates that client-side logic happened correctly
        - also where new data is available for other clients
           - which include realtime and conflicts
  - check couchDB and related for ideas about this
  - what about static assets???
     - some offline-first tools choose to bypass backend and let client directly interact with cloud provider

i18n

Privacy feature

Central BaaS API:
  - deploying backends using schema
  - schema file's user management (who can modify schema)
  - must submit not only schema but also directory/project around it:
     - because compiled schema still references files, including node modules
     - problem is size of hosting those directories
     - maybe should require GitHub repos, so no need to host

Sysadmin client app:
  - schema edition:
     - should perform client-side:
        - basic format validation, e.g. YAML linting following by YAML parsing
        - schema validation
        - schema test compilation
        - all this should reuse isomorphic server code
  - GUI to central BaaS API

Promotion:
  - commercial website
  - ads:
     - X-Powered-By [S]

Schema migrations:
  - guessing an schema from existing database.

Positioning:
  - main keywords: BaaS, featureful, easy, generic, stable, open-source
  - market: BaaS
  - main value: backend that is both featureful and easy to maintain
  - target audience:
     - developers, not newbyes
     - no assumptions on particular technologies or business cases
  - main requirements, in order:
     - easy:
        - maintainability: maintaining, setting up, upgrading, integrating, extending
        - manageability: operating, deploying, scaling, monitoring
        - learnability: documentation, support
        - UI dashboard: good UX, design, usability
     - stable:
        - tested, secure, reliable, available, recoverable
     - featureful:
        - any feature a backend can provide
        - high quality design/implementation of each feature
     - generic/agnostic:
        - prefer generic over specific, even it lowers efficiency or performance:
           - i.e. interoperability with specific tools (client libraries, databases, etc.) is not paramount
             although nice to have
        - flexibility:
           - allow customizing business logic, with least assumptions about it
           - do not allow end-users customizing API design:
              - prefer forcing good API design over flexibility
              - but encourage contributors to customize API design through generic plugin architecture
  - configuration:
     - featureful, i.e. many configuration options, which is ok
     - but easiness achieved thanks to:
        - minimal API surface for each option, by sacrificing specificity/efficiency over genericity
        - each option should have good default so they rarely need to be used
   - open source:
      - i.e. no vendor lock-in

 25  MUST HAVE FEATURES
 2   Authorization
 2   Orphans
 2   Pagination
 1   Complex patches
 1   Atomicity/rollbacks
 1   REST
 0.5 JSON-RPC
 4   Aggregating
?7   Compatibility layer
?7   Versioning/changes
 4   Static assets
 1   Exception handling
 1   Error reporting
 0.5 JSON schema $data
 1   inputValidation

 10  DATA MODEL
?5   Schema strictness/polymorphism
?3   API base types
 2   Upgrade MongoDB doc

 15  NICE TO HAVE FEATURES
?3   Async actions/tasks
?0.5 Alternate ids
?0.5 Server routing
?4   Realtime
?3   Concurrency conflicts
 1   Rate limiting
 3   Security
 0.5 Request timeout fix
 0.5 Multiple databases

 10  NOT ESSENTIAL FEATURES
 1   File formats
 2   HTTPS
 2   HTTP/2
 0.5 Proxies
 0.5 HTTP details
 2   GraphQL
 0.5 create dry runs
 1   Model naming
?3   Database transformation layer
?2   Custom actions
 1   Schema functions
 2   Compile-time validation
?3   Custom code
?1   Callbacks/events
 0.5 CLI
 1   systemInfo
 0.5 RegExp compatibility

 55  SOFTWARE QUALITY
 1   code quality
?5   Caching
?5   Optimization
 0.5 Limits
 1   Streaming
 30  Testing
 2   CI/CD
 6   Logging/monitoring
 2   Dependencies
 2   Less code
 1   ES modules
 0.1 Nodemon exit

 70  DEVOPS/LIVE
 2   Live database
?10  DevOps
?2   System routing
?20  Authentication
?10  Client
 2   GraphQL relay
?5   CLI tool
?25  Admin dashboard

 40  DEV FEATURES
 3   Debugging
 5   User documentation
 10  API documentation
 20  Thorough dev documentation
?2   Fake server
?3   Meta-information

 20  NOT IMPORTANT FEATURES
 *   Other database adapters
 *   Other operations
 3   Output format
 3   Node.js version
 10  Other programming languages
?    Schema format
?    Server-client state

?    Offline-first
?    i18n
?    Privacy feature
?    Central BaaS API
?    Sysadmin client app
?    Promotion
?    Schema migrations
?    Positioning
